{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a56b6d4c",
   "metadata": {},
   "source": [
    "E-Commerce: Product Range Analysis\n",
    "\n",
    
    "**Task:**\n",
    "\n",
    "****Analyze the store's product range.\n",
    "\n",
    "- Carry out exploratory data analysis\n",
    "- Analyze the product range\n",
    "- Formulate and test statistical hypotheses\n",
    "\n",
    "**Description of the data:**\n",
    "\n",
    "The dataset contains the transaction history of an online store that sells household goods.\n",
    "\n",
    "The file `ecommerce_dataset_us.csv` contains the following columns:\n",
    "\n",
    "`InvoiceNo` — order identifier\n",
    "\n",
    "`StockCode` — item identifier\n",
    "\n",
    "`Description` — item name\n",
    "\n",
    "`Quantity`\n",
    "\n",
    "`InvoiceDate` — order date\n",
    "\n",
    "`UnitPrice` — price per item\n",
    "\n",
    "`CustomerID`\n",
    "\n",
    "\n",
    "   * Who is our internal customer?\n",
    "project managers who are responsible for the relevance of the product range\n",
    "\n",
    "   * Why do they need that?\n",
    "they want to determine which products are included in the main and additional assortment in order to competently offer additional products to buyers and optimize purchases (there is no point in purchasing a lot of additional products when the main product is not available)\n",
    "Classifying or categorization can be one of the options for that\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Submitting the Project:\n",
    "\n",
    "Your project will have three components:\n",
    "1. A notebook containing your code (.ipynb)\n",
    "2. A presentation (.pdf)\n",
    "3. A link to your dashboard on Tableau Public (dashboard.txt) (optional)\n",
    "\n",
    "Make sure the solution is complete and check that the code works in Jupyter before submitting it for review. \n",
    "Don't be surprised if you're asked to make some improvements.\n",
    "Upload the archive to Google Drive and copy the shareable link to the beginning of your notebook.\n",
    "If you're planning to make a dashboard later, tell your team leader when you submit the project for the first time. \n",
    "The project will be accepted only after you submit your dashboard.\n",
    "Good luck!\n",
    "\n",
    "\n",
    "\n",
    "some of the source links: \n",
    "http://brandonrose.org/clustering\n",
    "\n",
    "http://datanongrata.com/2019/04/27/67/\n",
    "\n",
    "https://realpython.com/k-means-clustering-python/\n",
    "\n",
    "http://datanongrata.com/2019/04/27/67/\n",
    "\n",
    "https://medium.com/ssense-tech/unsupervised-product-clustering-exploring-the-cold-start-problem-8053ef04bac9\n",
    "\n",
    "https://medium.com/web-mining-is688-spring-2021/using-k-means-to-segment-customers-based-on-rfm-variables-9d4d683688c8\n",
    "\n",
    "https://towardsdatascience.com/rfm-segmentation-using-quartiles-and-jenks-natural-breaks-924f4d8baee1\n",
    "\n",
    "https://stackabuse.com/association-rule-mining-via-apriori-algorithm-in-python/\n",
    "\n",
    "https://www.nextlytics.com/blog/machine-learning-in-customer-segmentation-with-rfm-analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a59ef7",
   "metadata": {},
   "source": [
    "# Table of Contents:\n",
    "\n",
    "<div style=\"height:10px;\"></div>\n",
    "\n",
    "1. [Data Overview and Preprocessing:](#-Preprocessing)\n",
    "   * [Download the data, Renaming Column Names, Adding necessary column and Study the general information (using info(), describe..)](#-info)\n",
    "   * [Check for missing values, duplicated rows and other abnormal data (such as: zero and negative values) and choose appropriate ways to deal with them. Chande data type if needed](#-missing)\n",
    "   * [Study the distribution and dispertion (outliers)](#-dist)\n",
    "   * [Trying to find color and size traits out of product names](#-color)\n",
    "\n",
    "\n",
    "<div style=\"height:5px;\"></div>\n",
    "\n",
    "2. [Product Description Analysis:](#-Analysis)\n",
    "   * [Create a corpus out of product descriptions.](#-corpus)\n",
    "   * [Clean up the corpus and removed stopwords etc,Eliminate grammatical variations via stemming](#-clean)\n",
    "   * [Create a Term-Frequency Inverse Document Frequency (TF-IDF) matrix.](#-tfidf)\n",
    "   * [Clculate from the TF-IDF the corpus distance matrix comparing the relative similarity.](#-distance)\n",
    "   * [Use the distance matrix to build a dendrogram from which the number of clusters will be detrmined](#-dendrogram)\n",
    "   * [Using Kmean to form the clusters](#-kmeans)\n",
    "   * [Study the term frequencies for each cluster.](#-terms_freq)\n",
    "   * [Based on the term frequencies, identify product category keywords for each cluster.](#-keywords)\n",
    "   * [Categorizing the products in the store differently in order to get result that enable better understanding of product range.](#-categorization)\n",
    "   * [Calculating monthly revenue and monthly cumulating revenue to detect the trend](#-revenue) \n",
    "   * [Splitting products by category and finding: 1. the leading categories regarding the number of items in each category 2. the leading categories in sales](#-split)\n",
    "   * [Finding the top ten selling products](#-selling)\n",
    "   * [Examining Refunds: by total amount and by frequency](#-refunds)\n",
    "\n",
    "   \n",
    "    \n",
    "<div style=\"height:5px;\"></div>\n",
    "\n",
    "3. [Product Bundle(Basket) Analysis and Recommender System:](#-recommender)\n",
    "   * [Create a basket for each transaction (invoice) and study the popullarity for products to appear (in any transaction/invoice) as well as for their different combinations by using Apriori Algorithm](#-basket)\n",
    "   * [Calculate other Basket metrics using association_rule method in order to realize how to expand sails in the basket level by recommending additional products](#-additional)\n",
    "   * [Studying product similarities in order to realize how to mantain and increase profits (in a basket level) by recommanding sustitutional/interchangable products](#-interchangable)\n",
    "\n",
    "\n",
    "<div style=\"height:5px;\"></div> \n",
    "\n",
    "4. [Product Market Values Analysis:](#-market)\n",
    "   * [Calculate product recency-frequency-monetary (RFM Metrics)](#-rfm)\n",
    "   * [Study product RFM distributions (in order to determine the segmentation split)](#-split)\n",
    "   * [Categorize products based on K-MEANS](#-categorize)\n",
    "   * [Charactarize the product clusters based on R, F and M scores](#-RFM_scores)\n",
    " \n",
    "   \n",
    "<div style=\"height:10px;\"></div>\n",
    "\n",
    "\n",
    "5. [Conclusions and Suggestions:](#-conclusions)\n",
    " \n",
    "<div style=\"height:10px;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8665b3d1",
   "metadata": {},
   "source": [
    "<b id=\"-Preprocessing\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40c6116",
   "metadata": {},
   "source": [
    "## Data Overview and Preprocessing:\n",
    "\n",
    "* Download the data, Renaming Column Names, Adding necessary columns and  Study the general information (using info(), describe..)  \n",
    "* Check for missing values, duplicated rows and other abnormal data (such as: zero and negative values) and choose appropriate ways to deal with them. Chande data type if needed.\n",
    "* Study the distribution and dispertion (outliers)\n",
    "* Trying to find color and size traits out of product names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce82682",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downdload libraries:\n",
    "# delete dup import: !! \n",
    "\n",
    "# get all imports at the top of the project (in one cell) for more comfort and readability\n",
    "! pip install sidetable\n",
    "! pip install plotly -U\n",
    "! pip install altair -U\n",
    "#! pip install usaddress\n",
    "! pip install -U kaleido\n",
    "# ! pip install -m nltk.downloader stopwords\n",
    "! pip install --user -U nltk\n",
    "\n",
    "#nltk.download()\n",
    "! pip install nltk\n",
    "! pip install apyori\n",
    "! pip install mlxtend\n",
    "# rfm\n",
    "!pip install -U scikit-learn \n",
    "# library for machine lerning\n",
    "\n",
    "# library Roy added because he didn't have them\n",
    "! pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63d1152",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "\n",
    "#from __future__ import division\n",
    "import re\n",
    "import os\n",
    "import codecs\n",
    "import sys\n",
    "import sidetable as stb\n",
    "import numpy as np\n",
    "import math as mth\n",
    "import pandas as pd\n",
    "\n",
    "import datetime as dt\n",
    "from datetime import datetime,  timedelta\n",
    "\n",
    "from textwrap import wrap\n",
    "#import usaddress # ?\n",
    "#import altair as alt\n",
    "\n",
    "import plotly.offline as py\n",
    "import plotly.offline as pyoff\n",
    "import plotly as py\n",
    "import plotly.graph_objects as go\n",
    "from plotly import tools\n",
    "import plotly.offline as py \n",
    "import plotly.express as px\n",
    "\n",
    "#import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, RandomForestRegressor\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler # for features in linear regression to be standardized\n",
    " # metric for regression\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "# Use silhouette coefficient to determine the best number of clusters\n",
    "from sklearn.metrics import silhouette_score\n",
    "# RFM-segmentation using K-means\n",
    "import sklearn.cluster\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage \n",
    "from scipy import stats as st\n",
    "import scipy.stats as stats\n",
    "\n",
    "#import mpld3\n",
    "\n",
    "# crating distance matrix : use tf idf for clustering:\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn import feature_extraction\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "import nltk\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "# load nltk's SnowballStemmer as variabled 'stemmer'\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "from pymystem3 import Mystem\n",
    "from collections import Counter\n",
    "#from nltk.tokenize import word_tokenize\n",
    "#libraries for Apriori\n",
    "from mlxtend.frequent_patterns import apriori\n",
    "from mlxtend.frequent_patterns import association_rules\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning) #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ff4ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the data for missing values , their rashio ad other possible errors:\n",
    "def preprocess_df(dataframe: pd.core.frame.DataFrame, title: str):\n",
    "    print(f'df: {title}')\n",
    "    # look at a random sample of the data and study the information given:\n",
    "    display(dataframe.sample(10))\n",
    "    print()\n",
    "    dataframe.info(memory_usage='deep')\n",
    "    print()\n",
    "    display(dataframe.describe().T)# include ='all'\n",
    "    display(dataframe.describe(include ='object').T)\n",
    "    #display(dataframe.describe(include ='all').T)\n",
    "    print()\n",
    "    # Using sidetable method to look at the missing values per column, how many there are and their share (presentage).\n",
    "    display(dataframe.stb.missing(style = True))\n",
    "    print()\n",
    "    print(f'There are {dataframe.duplicated().sum()} duplicated rows')\n",
    "    print('\\n'*2)\n",
    "    for colname in dataframe.columns:\n",
    "        print(f'column: {colname.upper()}')\n",
    "        #print(f'The frequencies of unique values of column {colname.upper()} in descending order are:')\n",
    "        #display(dataframe.stb.freq([colname], style = True, cum_cols = False))\n",
    "        print(f'The unique values of column {colname.upper()} are:')\n",
    "        display(dataframe[colname].unique())\n",
    "        print(f'The number of NULL values in {colname} are:')\n",
    "        display(dataframe[colname].isnull().sum())\n",
    "        #print('share of missing values regarding their column' in order to figure out how significant their absence is for their category or column \n",
    "        print(f'Null values ratio: {round(dataframe[colname].isnull().sum() * 100 / len(dataframe[colname]),3)} %')\n",
    "        if dataframe[colname].isnull().sum() > 0 :\n",
    "            print(f'The rows with the missing values for {colname} column are the following:')\n",
    "            display(dataframe[dataframe[colname].isnull()])\n",
    "        display(dataframe[colname].describe(include ='all').to_frame().T)\n",
    "        print('\\n'*3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74e55b6",
   "metadata": {},
   "source": [
    "<b id=\"-info\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f608e2",
   "metadata": {},
   "source": [
    "### Download the data, Renaming Column Names, Adding necessary columns and Study the general information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9670db8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the file at the top of the page\n",
    "try:\n",
    "    df = pd.read_csv('ecommerce_dataset_us.csv' ,parse_dates=['InvoiceDate'], sep = '\\t').copy() \n",
    "except:\n",
    "    df = pd.read_csv('/datasets/ecommerce_dataset_us.csv', parse_dates=['InvoiceDate'], sep = '\\t').copy() \n",
    "\n",
    "    \n",
    "# Replace the column names (make them lowercase and add '_' between the words for a column name that consists more than one word):\n",
    "df.columns = df.columns.str.replace('No','_no')\n",
    "df.columns = df.columns.str.replace('Date','_datetime')\n",
    "df.columns = df.columns.str.replace('Code','_code')\n",
    "df.columns = df.columns.str.replace('Price','_price')\n",
    "df.columns = df.columns.str.replace('ID','_id')\n",
    "# Make all column names lowercase:\n",
    "df.columns = df.columns.str.lower() \n",
    "# Make all description values lowecase\n",
    "df['description'] = df['description'].str.lower() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b07077a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['invoice_datetime'] = pd.to_datetime(df['invoice_datetime'])\n",
    "#df['invoice_year_month'] = df['invoice_datetime'].map(lambda date: 100*date.year + date.month)\n",
    "#df['date'] = df['invoice_datetime'].dt.strftime('%Y-%m')\n",
    "\n",
    "df['invoice_period']= df['invoice_datetime'].apply(lambda x: x.strftime('%Y-%m'))\n",
    "\n",
    "# Create seperate columns to date and time:\n",
    "df['invoice_date']=pd.to_datetime(df['invoice_datetime']).dt.normalize()\n",
    "\n",
    "# Adding another column for total order price: \n",
    "df['tot_order'] = df['quantity']*df['unit_price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a69c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.rename(columns={\"invoice_date\": \"invoice_dt\"}, inplace = True)\n",
    "preprocess_df(df, 'Ecommerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ecb2d5",
   "metadata": {},
   "source": [
    "Conclusions:\n",
    "\n",
    "From the info() method it's turned out that 541,909 rows and 7 columns.\n",
    "There are 135,080 missing values in 'CustomerID' column, which representing 24.93% of the data and 1,454 missing values in 'Description' column representing 0.27% of the data. \n",
    "There are none missing values in other columns. \n",
    "\n",
    "\n",
    "Note: As for this project, regarging product analysis the information about 'customer_id' is irrelevant (due to thr fact that the products and not the customers are under examination. \n",
    "That's why , while it is indeed important to inform the managers in charge (and the stuff) about it in order to figure out its source, it can be ignored. There is no reason to delete those rows or think of the probber technique how to deal whith them.\n",
    "As for the missing values in 'description' column - they can be omitted (deleted) since their share is minor. (0.27%)\n",
    "\n",
    "There are 5268 duplicated rows. \n",
    "\n",
    "\n",
    "Data Types:\n",
    "invoice_no, stock_code and description are of object type, unit_price and customer_id are floats, invoice_datetime is datetime and quantity is an int.  \n",
    "\n",
    "       \n",
    "The file columns contain the following information:\n",
    "   * 'invoice_no' — order identifier\n",
    "   * 'stock_code' — item identifier\n",
    "   * 'description' — item name \n",
    "   * 'quantity' — \n",
    "   * 'invoice_datetime' — order date\n",
    "   * 'unit_price' - price per item\n",
    "   * 'customer_id' - customer identifier\n",
    "\n",
    "\n",
    "Appearantly, 'white hanging heart t-light holder' is the most popular product among 4206 unique ones, with frequency of 2369 units. \n",
    "\n",
    "Quantity has negative and extreme (low and high) values that will be considered later on. unit_price also has the similar problem and will be treated as well. \n",
    "\n",
    "Invoice datetime lasts from 2018-11-29 untill 2019-12-07 so that invoice period includes 14 month, meaning one year and two months from 2019-11\t86074."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221ff0b0",
   "metadata": {},
   "source": [
    "<b id=\"-missing\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a739971",
   "metadata": {},
   "source": [
    "### Check for missing values, duplicated rows and other abnormal data  and choose appropriate ways to deal with them. Change data type if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9983dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To conclude: \n",
    "# Missing Values:\n",
    "display(df.stb.missing(style = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e3782f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# looking for duplicates:\n",
    "dup_no = df.duplicated().sum()\n",
    "if dup_no > 0:\n",
    "    display(dup_no)\n",
    "    dup_ratio = round(df.duplicated().sum() * 100 / len(df),2)\n",
    "    print(f'Duplicated rows ratio: {dup_ratio} %')\n",
    "    display(df[df.duplicated()])\n",
    "    print()\n",
    "    # duplicated rows can point out problems in the products and data. That's why it's important to check where exactly we have problems. \n",
    "    # I would also check when the duplicates appeared? Maybe it was a certain date. I had a problem like that\n",
    "\n",
    "    # if there are dup - something happenrd in the logging process ('gilch')- the  problem in the server\n",
    "    # what to de with it: -check when and where (what columns, for what users, what groups have dup)it occurs\n",
    "    # if it happen only begginning of test ,for 2 hours for 20 users we can delete them\n",
    "    # but here, to all event , for 237, the whole week in three groups. --> go to programmers and engeneers  ask what happened ,is the data is ..was the glich i the database(some disconnection to the database. so the server tried to send same info again) side or the server\n",
    "    # in that case - no biggie . cause te data is safe -remove the dup. otherwise - the time icorrect or.. -then maybe the data cannot be trusted/ \n",
    "    # dup can influence the integrity of the overall data and test\n",
    "\n",
    "for i in df[df.duplicated()].columns:\n",
    "    print(i,':', df[df.duplicated()][i].nunique())\n",
    "\n",
    "    # Let's figure out maybe all duplicates were created on the same problematic day or maybe two (since 'invoice_date' include both time  and date, the fact that there are 352 uniques doesn't mean that it's impossible that all of them occur on the same date)\n",
    "    display(df[df.duplicated()]['invoice_date'].dt.date.unique())\n",
    "\n",
    "    # Unfortunately, the duplicates creation date is absolutely random !\n",
    "\n",
    "\n",
    "# Drop duplicated rows : (less then 1% )\n",
    "df = df[~(df.duplicated())].reset_index(drop=True)\n",
    "display(df.duplicated().sum())\n",
    "\n",
    "# preprocess_df(df, 'Ecommerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589f64f7",
   "metadata": {},
   "source": [
    "Conclusion:\n",
    "\n",
    "There are 5268 duoplicated rows. \n",
    "Duplicated rows ratio is:  0.97 %, a minor share of the data , so it can be deleted especialy since after examining the dates it does not seemed to occur in some \"proplematic\" date so it might be routine malfunction (technical). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04bcdfa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping all rows with null in descriprion. it's only 0.2% and ..:\n",
    "df = df[df['description'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875fd94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('negative quantity:') # 9762  neg\n",
    "neg_quantity_df = df.query('quantity < 0')\n",
    "display(neg_quantity_df)# 9725 neg\n",
    "#display(list(neg_quantity_df['description'].unique()))\n",
    "#print()\n",
    "#print('negative quantity description:')\n",
    "#display(df.query('quantity < 0').describe().T)# min: -80995.0, max:-1. (weired desc: discount, manual)\n",
    "print('zero quantity:')\n",
    "display(df.query('quantity == 0'))# no zero quantity \n",
    "#print('zero quantity description::')\n",
    "#display(df.query('quantity == 0').describe().T)\n",
    "print()\n",
    "\n",
    "print('descriptions of negative/zero price :')\n",
    "neg_zero_unit_price_df = df.query('unit_price < 0 or unit_price ==0') # only 414 rows - zero unit price\n",
    "display(neg_zero_unit_price_df)\n",
    "#display(list(neg_zero_unit_price_df['description'].unique()))\n",
    "\n",
    "\n",
    "# no more quantity zero or netative!!!!!!!!!!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8875d2",
   "metadata": {},
   "source": [
    "Conclusion:\n",
    "\n",
    "There are 9,725 negative values in 'quantity' column and no zero values. \n",
    "\n",
    "As for 'unit_price column', there are  1,058 values wiyh negative or zero values. Those rows also share descriptions that imdicate the invoice does not represent sales."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ee0dae",
   "metadata": {},
   "source": [
    "<b id=\"-dist\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0959ac",
   "metadata": {},
   "source": [
    "### Study the distribution and dispertion (outliers):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c391cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How about outliers in the data? Are there abnormally large order amounts?..\n",
    "\n",
    "#A look at the distribution for quantity and unit price show that the majority of values are in single digits. \n",
    "# There however seems to be outliers of up to 80,995 for order quantity and 38,970 for unit price.\n",
    "\n",
    "plt = sns.distplot(df['quantity'].value_counts(), kde = True)\n",
    "plt.set_title('Quantity Distribution:')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d222d1",
   "metadata": {},
   "source": [
    "Conclusion:\n",
    "\n",
    "Seems like most of Quantity records are concentrated around the pretty low values to the right of zero, but the distribution is skewed to the right.. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eccc491e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt = sns.distplot(df['unit_price'].value_counts(), kde = True)\n",
    "plt.set_title('Unit Price Distribution:')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a747085",
   "metadata": {},
   "source": [
    "Conclusion:\n",
    "    \n",
    "Seems like most of Unit Price records are concentrated in low values to the right of zero, but the distribution is skewed to the right.. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f605203",
   "metadata": {},
   "source": [
    "Conclusion:\n",
    "\n",
    "Seems like most of Unit Price records are concentrated in low values to the right of zero, but the distribution is skewed to the right.. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbefdaae",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt = sns.distplot(df['tot_order'].value_counts(), kde = True)\n",
    "plt.set_title('Total Order Distribution:')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf61703",
   "metadata": {},
   "source": [
    "Conclusion:\n",
    "    \n",
    "Seems like most of Total Orer records are concentrated in low values to the right of zero, but the distribution is skewed to the right.. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23defa98",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df[['quantity', 'unit_price', 'tot_order']].describe().T)\n",
    "print()\n",
    "\n",
    "# outliers: \n",
    "print('unit_price destribution')\n",
    "display(df.stb.freq(['unit_price'], style = True, cum_cols = False))# cum_cols=false otherwiseit sum up the persentages\n",
    "print('quantity destribution')\n",
    "display(df.stb.freq(['quantity'], style = True, cum_cols = False))# cum_cols=false otherwiseit sum up the persentages\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2afad8a1",
   "metadata": {},
   "source": [
    "Conclusion:\n",
    "\n",
    "- There are records with UnitPrice<0 and Quantity<0. We need to remove them from the analysis.\n",
    "- The min and max value for Quantity is 80995, this could represent cancelled or returned orders.\n",
    "- The UnitPrice also have few negative values which are uncommon, these transactions could represent cancelled orders by customers or bad debt incurred by the business.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70cbfcd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets run a quick filter for quantity-total_order outliers: \n",
    "print(\"quantity-total_order outliers(quantity > 1,000 and total order > 5,000):\")\n",
    "display(df[(df['quantity'] > 1000) & (df['tot_order'] > 5000)])\n",
    "\n",
    "print(\"Looking for a specific outlier with total order of -168469.600000\")\n",
    "display(df[df['tot_order'] == -168469.600000] )# 2 only\n",
    "\n",
    "# What about the unit price outliers? A look at the highest mean values bring up some suspect \"products\":\n",
    "# DOTCOM POSTAGE\n",
    "# CRUK Commission\n",
    "# Manual\n",
    "# Discount\n",
    "\n",
    "df.groupby('description').mean()['unit_price'].nlargest()\n",
    "\n",
    "# Let's examine postage related data in particular: \n",
    "# Creating a list of dotcome postage(/amazon) fee :\n",
    "postage_dotcome_amazon_list = [\n",
    "'dotcom sales','dotcom sold sets', 'postage', 'dotcom set', 'dotcom postage','sold as set/6 by dotcom','sold as set on dotcom','sold as set on dotcom and amazon',\n",
    "'dotcome sales','sold as set on dotcom', 'sold as set on dotcom and amazon', 'sold as set by dotcom','sold as 1 on dotcom', 'dotcom adjust','allocate stock for dotcom orders ta',\n",
    "'dotcom set','sold as set on dotcom','amazon sold sets','dotcom sold sets', 'sold as 1 on dotcom', 'dotcom sold sets',\"dotcom sold in 6's\", 're dotcom quick fix.',\n",
    "'dotcomstock','dotcom adjust','rcvd be air temp fix for dotcom sit', 'amazon sold sets','amazon fee','amazon sales','amazon','amazon sold sets','amazon adjust', \n",
    "'amazon adjustment','sold as set on dotcom and amazon','sold as set on dotcom and amazon', 'ebay', 're dotcom quick fix.'\n",
    "]\n",
    "# Subseting df with those descriptions:\n",
    "postage_dotcome_amazon_descriptions_df = df[df['description'].isin(postage_dotcome_amazon_list)]\n",
    "print('Dotcome Postage/Amazon Fee related df rows:')\n",
    "display(postage_dotcome_amazon_descriptions_df)\n",
    "print(\"Quantity values of Dotcome Postage/Amazon Fee related df:\")\n",
    "display(postage_dotcome_amazon_descriptions_df['quantity'])\n",
    "print(\"Unit Price values of Dotcome Postage/Amazon Fee related df:\")\n",
    "display(df[df['description'] == 'dotcom postage']['unit_price'].describe())\n",
    "\n",
    "# filter postage related descritions:  \n",
    "df = df[~df['description'].isin(postage_dotcome_amazon_descriptions_df)]\n",
    "\n",
    "# Now, let's examine 'CRUK Commission' - charity related data in particular: \n",
    "print('CRUK Commission -Charity related df rows:')\n",
    "display(df[df['description'] == 'cruk commission'])\n",
    "\n",
    "# Now, let's examine 'Manual' data in particular: \n",
    "\n",
    "# Creating list with manual related descriptions:\n",
    "\n",
    "manual_damaged_products_related_list = [\n",
    "                 'dotcom postage','thrown away-rusty','party bunting','wet/rusty','damages/dotcom?','on cargo order','smashed','wet damaged',\n",
    "                 'water damaged','printing smudges/thrown away','to push order througha s stock was ','found some more on shelf','show samples',\n",
    "                 'mix up with c', 'wrongly marked. 23343 in box','alan hodge cant mamage this section','fba','stock creditted wrongly','incorrectly put back into stock',\n",
    "                 'manual','damages/samples','sold as 1 on dotcom','key fob , shed','key fob , back door ','code mix up? 84930','?display?','sold as 1',\n",
    "                 '?missing','crushed ctn','test','temp adjustment','taig adjust','allocate stock for dotcom orders ta', '??','add stock to allocate online orders',\n",
    "                 'for online retail orders','found box','oops ! adjustment','found in w/hse','website fixed','dagamed','historic computer difference?....se','incorrect stock entry.',\n",
    "                 'michel oops', 'wrongly coded 20713','wrongly coded-23343','stock check','crushed boxes','wet/mouldy','wet/rusty','mailout',\"can't find\",'mouldy','wet pallet-thrown away',\n",
    "                 'had been put aside.','sale error','wrongly marked 23343','20713 wrongly marked','re-adjustment','breakages','marked as 23343','20713',\n",
    "                 'wrongly coded 23343','found by jackie','unsaleable, destroyed.','wrongly marked','had been put aside','damages wax',\n",
    "                 'wrongly mrked had 85123a in box','wrongly marked carton 22804','missing?', 'wet rusty', '?lost', '?lost?', 'lost?',\n",
    "                 'rusty thrown away','check?', '?? missing','wet pallet','????missing','lost in space','wet?', 'lost??','???','wet boxes',\n",
    "                  '????damages????','mixed up','lost','given away','label mix up','samples/damages','thrown away','adjustment','wrongly sold as sets','wrongly sold sets',\n",
    "                  '? sold as sets?', '?sold as sets?', 'thrown away.','damages/display',\n",
    "                  'damaged stock','broken','throw away','wrong barcode (22467)','wrongly sold (22719) barcode','wrong barcode', 'barcode problem',\n",
    "                  '?lost','found','faulty','?','check','damages','cracked','sold as 22467','sold in set?','damages?', 'key fob , front  door ',\n",
    "                  'damages/display','damaged stock','broken','throw away','wrongly sold sets','? sold as sets?','?sold as sets?', 'wrongly sold as sets',\n",
    "                  'amazon sold sets','samples/damages',  'label mix up', 'faulty','manual','amazon fee',\n",
    "                  'bank charges','check', 'damages','samples', '?','discount','damages/showroom etc','adjust', 'crushed','returned','display',\n",
    "                  'cracked','sold as 22467','incorrectly made-thrown away.', \"thrown away-can't sell.\", \"thrown away-can't sell\",\n",
    "                  'wrongly sold sets','? sold as sets?','?sold as sets?', 'found','counted', 'returned', '???missing','mouldy, thrown away.', 'damaged' ,\n",
    "                  'wet','missing','reverse 21/5/10 adjustment','damaged','reverse 21/5/10 adjustment',\n",
    "                  'mouldy, thrown away.','showroom','mia','mystery! only ever imported 1800','merchant chandler credit error, sto','possible damages or lost?',\n",
    "                  'display', 'missing','wrong code?','wrong code','damages/credits from asos.','reverse previous adjustment','incorrectly credited c550456 see 47',\n",
    "                  'mouldy, unsaleable.','taig adjust no stock','???lost','sold with wrong barcode','rusty throw away','adjust bad debt','did  a credit  and did not tick ret',\n",
    "                  'mailout ','dotcomstock','rcvd be air temp fix for dotcom sit'\n",
    "]\n",
    "\n",
    "\n",
    "# Subseting df with those descriptions:\n",
    "manual_related_descriptions_df = df[df['description'].isin(manual_damaged_products_related_list)]\n",
    "print('Dotcome Postage/Amazon Fee related df rows:')\n",
    "display(manual_related_descriptions_df)\n",
    "print (\"Unit Price Manual related df\")\n",
    "display(manual_related_descriptions_df['unit_price'].describe())\n",
    "print()\n",
    "print(\"Manual related df rows with manual description and tot order above 3000\")\n",
    "display(manual_related_descriptions_df[manual_related_descriptions_df['tot_order'] > 3000])\n",
    "print()\n",
    "\n",
    "# filter manual related descritions:  \n",
    "df = df[~df['description'].isin(manual_damaged_products_related_list)]\n",
    "\n",
    "#  Delete all descriptions with '?':\n",
    "bool_vec = ~df['description'].str.contains('?',na=False,regex=False)\n",
    "display(bool_vec)\n",
    "display(df.loc[bool_vec])\n",
    "df = df.loc[bool_vec]\n",
    "\n",
    "print (\"Unit Price Discount related df\")\n",
    "display(df[df['description'] == 'discount']['unit_price'].describe())\n",
    "print()\n",
    "\n",
    "print(\"Discount related df rows:\")\n",
    "display(df[df['description'] == 'discount'])\n",
    "print()\n",
    "\n",
    "print(\"removed categories\")\n",
    "removed_cats = ['dotcom postage related', 'cruk commission', 'manual related']\n",
    "display(removed_cats)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600173f6",
   "metadata": {},
   "source": [
    "Conclusion:\n",
    "\n",
    "Outliers: \n",
    "Some outliers have been detected in unit_price and quantity columns that should be treated to prevent any destortion in the analysis results later on. \n",
    "\n",
    "By running a quick filter, we see that the outliers (5 only) mostly lie in small ticket items that cost around 1-2 pounds each. \n",
    "The most obvious outlier would be invoice number 581483. \n",
    "It seems that someone really really likes crafted paper birdies. Or... after looking for the same negative value for total order and the same (but negative) quantity of the same stock_code with 'paper craft , little birdie' was indeed detected. \n",
    "\n",
    "Judging by this cancellation, maybe not so much ...\n",
    "\n",
    "There doesn't seem to be any other entries within the outliers to suggest skewing the sales history especially when some of the largest tickets were all cancelled.\n",
    "\n",
    " - 'DOTCOM POSTAGE'/'DOTCOME','AMAZON' related descriptions seems to indicate the amount spent by the customer on postage. With an average of 709, this is over 200 times the average unit price of all products. \n",
    "   Furthermore, postage isn't a direct indicator of sales and might skew the amount spent across cohorts. (There also seems to be a similar label containg 'POSTAGE' or wlse which we will remove as well.)\n",
    "\n",
    " - 'CRUK Commission' sounded like a fee paid out to an external organization and a quick google search turned up related results at Cancer Research UK. \n",
    "    This might be part of an initiative to pay out some proceeds to the cancer research effort. As this isn't directly related to sales, we should drop 'CRUK Commission' rows.\n",
    "    (Looking up on line I figure that 'Cancer Research UK' (CRUK) is the world's largest independent cancer research organization. It is registered as a charity in the United Kingdom.)\n",
    "    \n",
    " - 'Manual' is a product that is rather nebulous. There are 567 records of Manual, with a single record commanding an average unit price of 72.48. \n",
    "    As we do not have specific information regarding the operations of this online retail company, perhaps 'Manual' refers to manual services rendered with the purchase of other items. \n",
    "   This could be the setting up of chandeliers or the shelves in a warehouse. \n",
    "   As there are significant outliers for these transactions that do not directly tie to individual product sales, we will drop 'Manual' records too.\n",
    "\n",
    "- 'Discount' seems to be a rather self explanatory category for discounts offered for products sold. \n",
    "   This is further corroborated by the fact that all these transactions have negative sales quantities. \n",
    "   As discounts directly tie into the price of products and impact sales directly, we will leave it in the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d31b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's deal with other possible problems in product names (description column):\n",
    "# function for dealind with prefix in invoice_no that means \"canceled\":\n",
    "\n",
    "df['is_refund']=df['invoice_no'].apply(lambda x: True if \"C\" in x else False )\n",
    "refunds_df_cancelations =df[df['is_refund']]\n",
    "display(df[df['is_refund']])# 8787 rows of  refunds - to be droped (less then 2%)\n",
    "display(any(df[df['is_refund']]['quantity'].values>0))# False\n",
    "\n",
    "# I have tried to make a function to look for \"pairs\" for refunds (delete the purchase but it didn't work I couldnt distingwish the purchases)\n",
    "\n",
    "df = df[df['is_refund']==False]\n",
    "df.info()# 521713  rows now"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c178114",
   "metadata": {},
   "source": [
    "Conclusion:\n",
    "\n",
    "products with invoice number that has prefix \"C\" seems to be shorts for \"canceled\" or \"cancelation\" and perhaps do not represents sales. There are 8787 rows of refunds like those - to be droped (with minor shaer - less then 2% of the data)\n",
    "Moreover their quantity is negative and that also point out the same fact.\n",
    "\n",
    "Those rows should be ommitted from data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8b5801",
   "metadata": {},
   "source": [
    "Conclusion:\n",
    "    \n",
    "Descriptions with quastion marks probably do not represents sales and should be deleted from the data. (their share is minor)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995b1743",
   "metadata": {},
   "outputs": [],
   "source": [
    "problematic_descriptions_list = ['dotcom postage',\n",
    "                 'not rcvd in 10/11/2010 delivery',\n",
    "                 'thrown away-rusty',\n",
    "                 'party bunting',\n",
    "                 'sold as set/6 by dotcom',\n",
    "                 'wet/rusty',\n",
    "                 'damages/dotcom?',\n",
    "                 'on cargo order',\n",
    "                 'smashed',\n",
    "                 'wet damaged',\n",
    "                 'water damaged',\n",
    "                 'sold as set on dotcom',\n",
    "                 'sold as set on dotcom and amazon',\n",
    "                 'water damage',\n",
    "                 'sold as set on dotcom',\n",
    "                 'sold as set on dotcom and amazon',\n",
    "                 'sold as set by dotcom',\n",
    "                 'printing smudges/thrown away',\n",
    "                 'to push order througha s stock was ',\n",
    "                 'found some more on shelf',\n",
    "                  'show samples',\n",
    "                  'mix up with c', \n",
    "                  'wrongly marked. 23343 in box',\n",
    "                  'alan hodge cant mamage this section',\n",
    "                  'fba',\n",
    "                  'stock creditted wrongly',\n",
    "                  'incorrectly put back into stock',\n",
    "                  'ebay',\n",
    "                  'manual',\n",
    "                  'damages/samples',\n",
    "                  'sold as 1 on dotcom',\n",
    "                  'key fob , shed',\n",
    "                  'key fob , back door ',\n",
    "                  'code mix up? 84930',\n",
    "                  '?display?',\n",
    "                  'sold as 1',\n",
    "                  '?missing',\n",
    "                  'crushed ctn',\n",
    "                  'test',\n",
    "                  'temp adjustment',\n",
    "                  'taig adjust',\n",
    "                  'allocate stock for dotcom orders ta',\n",
    "                   '??',\n",
    "                  'add stock to allocate online orders',\n",
    "                  'for online retail orders',\n",
    "                  'found box','oops ! adjustment',\n",
    "                  'found in w/hse',\n",
    "                  'website fixed',\n",
    "                  'dagamed',\n",
    "                  'historic computer difference?....se',\n",
    "                   'incorrect stock entry.',\n",
    "                   'michel oops',\n",
    "                   'wrongly coded 20713',\n",
    "                   'wrongly coded-23343',\n",
    "                   'stock check',\n",
    "                   'crushed boxes',\n",
    "                   'wet/mouldy',\n",
    "                   'wet/rusty',\n",
    "                   'mailout',\n",
    "                   \"can't find\",\n",
    "                   'mouldy',\n",
    "                   'wet pallet-thrown away',\n",
    "                   'had been put aside.',\n",
    "                   'sale error',\n",
    "                   'amazon adjustment',\n",
    "                   'wrongly marked 23343',\n",
    "                   '20713 wrongly marked',\n",
    "                   're-adjustment',\n",
    "                   'breakages',\n",
    "                   'marked as 23343',\n",
    "                   '20713',\n",
    "                   'wrongly coded 23343',\n",
    "                   'found by jackie',\n",
    "                   'unsaleable, destroyed.',\n",
    "                   'wrongly marked',\n",
    "                   'had been put aside','damages wax',\n",
    "                   'wrongly mrked had 85123a in box',\n",
    "                   'wrongly marked carton 22804',\n",
    "                   'missing?',\n",
    "                   'wet rusty',\n",
    "                   'amazon adjust',\n",
    "                   '?lost', \n",
    "                   '?lost?', \n",
    "                   'lost?',\n",
    "                   'dotcom adjust',\n",
    "                   'rusty thrown away',\n",
    "                   'check?',\n",
    "                   '?? missing',\n",
    "                   'wet pallet',\n",
    "                    '????missing',\n",
    "                    'lost in space',\n",
    "                    'wet?',\n",
    "                    'lost??',\n",
    "                    '???',\n",
    "                    'wet boxes',\n",
    "                    '????damages????',\n",
    "                    'mixed up',\n",
    "                    'lost',\n",
    "                    'given away',\n",
    "                    'dotcom',\n",
    "                    'label mix up',\n",
    "                    'samples/damages',\n",
    "                    'thrown away',\n",
    "                    'adjustment',\n",
    "                    'dotcom set',\n",
    "                    'wrongly sold as sets',\n",
    "                    'amazon sold sets',\n",
    "                    'dotcom sold sets',\n",
    "                    'wrongly sold sets',\n",
    "                    '? sold as sets?',\n",
    "                    '?sold as sets?',\n",
    "                    'thrown away.',\n",
    "                    'damages/display',\n",
    "                    'damaged stock',\n",
    "                    'broken',\n",
    "                    'throw away','wrong barcode (22467)',\n",
    "                    'wrongly sold (22719) barcode',\n",
    "                    'wrong barcode',\n",
    "                    'barcode problem',\n",
    "                    '?lost','found','faulty','dotcom sales',\n",
    "                    'amazon sales','amazon',\n",
    "                    '?','check','damages','cracked',\n",
    "                    'sold as 22467', \"dotcom sold in 6's\",\n",
    "                    'sold in set?',\n",
    "                    'damages?', \n",
    "                    'key fob , front  door ',\n",
    "                    'damages/display',\n",
    "                    'damaged stock',\n",
    "                    'broken',\n",
    "                    'throw away',\n",
    "                    'wrongly sold sets',\n",
    "                    '? sold as sets?',\n",
    "                    '?sold as sets?',\n",
    "                    'dotcom set',\n",
    "                    'wrongly sold as sets',\n",
    "                    'amazon sold sets',\n",
    "                    'dotcom sold sets',\n",
    "                    'samples/damages',  \n",
    "                    'label mix up',\n",
    "                    'dotcom',\n",
    "                    'dotcom sales', \n",
    "                    'faulty',\n",
    "                    'manual',\n",
    "                    'amazon fee',\n",
    "                    'bank charges', \n",
    "                    'check',\n",
    "                    'damages',\n",
    "                    'samples', \n",
    "                    'postage', \n",
    "                    '?',\n",
    "                    'discount',\n",
    "                    'damages/showroom etc',\n",
    "                    'adjust',\n",
    "                    'crushed',\n",
    "                    'returned',\n",
    "                    'display',\n",
    "                    'cracked',\n",
    "                    'sold as 22467',\n",
    "                    'incorrectly made-thrown away.',\n",
    "                     \"thrown away-can't sell.\",\n",
    "                     \"thrown away-can't sell\",\n",
    "                    'wrongly sold sets',\n",
    "                    '? sold as sets?',\n",
    "                    '?sold as sets?', \n",
    "                    'found',\n",
    "                    'counted', \n",
    "                    'returned',\n",
    "                    '???missing',\n",
    "                    'mouldy, thrown away.',\n",
    "                    'damaged' ,\n",
    "                    'wet',\n",
    "                    'missing',            \n",
    "                    'reverse 21/5/10 adjustment',\n",
    "                    'damaged',\n",
    "                    'reverse 21/5/10 adjustment',\n",
    "                     'mouldy, thrown away.',\n",
    "                     'showroom',\n",
    "                     'mia',\n",
    "                     're dotcom quick fix.',\n",
    "                     'mystery! only ever imported 1800',\n",
    "                     'merchant chandler credit error, sto',\n",
    "                     'possible damages or lost?',\n",
    "                     'display',\n",
    "                     'missing',\n",
    "                     'wrong code?',\n",
    "                     'wrong code',\n",
    "                     'damages/credits from asos.',\n",
    "                     'reverse previous adjustment',\n",
    "                     'incorrectly credited c550456 see 47',\n",
    "                     'mouldy, unsaleable.',\n",
    "                     'taig adjust no stock',\n",
    "                     '???lost',\n",
    "                     'sold with wrong barcode',\n",
    "                     'rusty throw away',\n",
    "                     'adjust bad debt',\n",
    "                     'did  a credit  and did not tick ret',\n",
    "                     'mailout ','dotcomstock',\n",
    "                     'rcvd be air temp fix for dotcom sit']\n",
    "\n",
    "problematic_descriptions_df = df[df['description'].isin(problematic_descriptions_list)]\n",
    "display(problematic_descriptions_df)\n",
    "print(\"problematic_descriptions quantiry values:\")\n",
    "display(problematic_descriptions_df['quantity'])\n",
    "# filter descritions:  \n",
    "df_new = df[~df['description'].isin(problematic_descriptions_list)]\n",
    "df_new.info() # 534521\n",
    "\n",
    "# check the share of the lost..\n",
    "print('unfiltered data:')\n",
    "print(len(df))\n",
    "print('filltered data:')\n",
    "print(len(df_new))\n",
    "print('Share of lost data:')\n",
    "print(round(100-len(df_new)/len(df)*100,1),'%' )        \n",
    "df = df_new    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c62c0d",
   "metadata": {},
   "source": [
    "Conclusion:\n",
    "\n",
    "Bad debt adjustments together with other suspicious item names should be dropped from the dataset as these do not represent actual sales. \n",
    "\n",
    "Note: their share is only 2% of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee957f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('negative quantity:') # 9762  neg\n",
    "neg_quantity_df = df.query('quantity < 0')\n",
    "display(neg_quantity_df)# 9762 neg\n",
    "#display(list(neg_quantity_df['description'].unique()))\n",
    "#print()\n",
    "#print('negative quantity description:')\n",
    "#display(df.query('quantity < 0').describe().T)# min: -80995.0, max:-1. (weired desc: discount, manual)\n",
    "print('zero quantity:')\n",
    "display(df.query('quantity == 0'))# no zero quantity \n",
    "#print('zero quantity description::')\n",
    "#display(df.query('quantity == 0').describe().T)\n",
    "print()\n",
    "\n",
    "\n",
    "print('descriptions of negative/zero price :')\n",
    "neg_zero_unit_price_df = df.query('unit_price < 0 or unit_price ==0') # only 409 rows - zero unit price\n",
    "display(neg_zero_unit_price_df)\n",
    "#display(list(neg_zero_unit_price_df['description'].unique()))\n",
    "\n",
    "\n",
    "# no more quantity zero or netative!!!!!!!!!!!!\n",
    "\n",
    "\n",
    "display(df[df['unit_price']==0]['stock_code'].nunique()) # 192 unique stock code when 'unit_price' is zero and need to be corrected with zero \n",
    "#display(df[df['unit_price']==0]['stock_code'].nunique())\n",
    "display(df[df['unit_price']==0]) # 2515 rows with zero values .. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb0e294",
   "metadata": {},
   "source": [
    "    - The min and max value for Quantity is 80995, this could represent cancelled or returned orders.\n",
    "    - The UnitPrice also have few negative values which are uncommon, these transactions could represent cancelled orders by customers or bad debt incurred by the business.\n",
    "    - Bad debt adjustments will be dropped from the dataset as these do not represent actual sales.\n",
    "\n",
    "\n",
    "Removing the negative values from UnitPrice and Quantity:\n",
    "df = df[df.Quantity > 0]\n",
    "df = df[df.UnitPrice > 0]\n",
    "#Removing the Null values from the data.\n",
    "df = df[pd.notnull(df['CustomerID'])]\n",
    "\n",
    "Cleaning the Date Column:\n",
    "df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'])\n",
    "df['InvoiceYearMonth'] = df['InvoiceDate'].map(lambda date: 100*date.year + date.month)\n",
    "df['Date'] = df['InvoiceDate'].dt.strftime('%Y-%m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e733302",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dealing with negative/zero values in the data frame: \n",
    "\n",
    "# , so first, treat 0 values as missing - nan then..\n",
    "df.loc[(df['unit_price'] == 0), 'unit_price'] = np.nan\n",
    "#df.info() # 2515 'unit_price' rows are null now. (all zeroes indeed)\n",
    "# Then, it is possible to use 'fillna' methods:\n",
    "\n",
    "#df_single_products = df[df['quantity'] == 1]\n",
    "df_new = df.copy()\n",
    "df['unit_price'] = df.groupby(['stock_code'])['unit_price'].transform(lambda grp: grp.fillna(grp.mode()))\n",
    "display(df['unit_price'])\n",
    "display(df.info())\n",
    "# the rest - complete with median :\n",
    "df['unit_price'] = df.groupby(['stock_code'])['unit_price'].transform(lambda grp: grp.fillna(grp.median()))\n",
    "display(df.info())# only 1 without unit_price now:\n",
    "#display(df[df['unit_price'].isnull()]) # study them . look at their descriptions - they are all defected products that cannot be sell . \n",
    "# lets dropthem from df:\n",
    "df.dropna(subset= ['unit_price'], inplace=True)\n",
    "df.info() # from now on we continue the analysis with  540434 rows !!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32f356b",
   "metadata": {},
   "source": [
    "<b id=\"-color\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e6f7f1",
   "metadata": {},
   "source": [
    "### Trying to find color and size traits out of product names:       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5182d017",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying to analize color and size traits for the product:\n",
    "\n",
    "#Creating color list\n",
    "colors_list = ['red', 'rose','yellow', 'green', 'pink', 'blue', 'ivory', 'purple', 'white','black', 'silver', 'gold', 'turquoise' ,'color','brown','grey', 'gray' , 'orange', 'violet','colour','light' , 'dark' , 'pastel','black/blue','blue/green' , 'yellow/pink' ,'pink/white','pink/purple','pink/blue', ' silver/black', 'white/pink', 'black+white']\n",
    "# , '``', '*'\n",
    "        \n",
    "def get_color(desc):\n",
    "    for word in desc.split():\n",
    "        if word in colors_list:\n",
    "            #print(\"%s in string\" % word)\n",
    "            #print( word)\n",
    "            return word\n",
    "        return 'undefined'\n",
    "\n",
    "# Creating size list:\n",
    "size_list = ['large', 'small', 'mini', 'medium', 'size' ,'grand', 'giant', 'jumbo', 'cm', 'kg' ,'set']\n",
    "\n",
    "def get_size(desc):\n",
    "    for word in desc.split():\n",
    "        if word in size_list:\n",
    "            #print(\"%s in string\" % word)\n",
    "            #print( word)\n",
    "            return word\n",
    "        return 'undefined'\n",
    "\n",
    "# Add color and size columns :\n",
    "df['color'] = df['description'].apply(get_color)    \n",
    "display(df.stb.freq(['color'], style = True, cum_cols = False))\n",
    "df['size'] = df['description'].apply(get_size)    \n",
    "display(df.stb.freq(['size'], style = True, cum_cols = False))\n",
    "\n",
    "# It terned out that most of the descriptions lacks of those new traits so.. delete those rows of the data \n",
    "df.drop(\"size\", inplace=True, axis = 1)\n",
    "df.drop(\"color\", inplace=True, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306686ed",
   "metadata": {},
   "source": [
    "Conclusion:\n",
    "\n",
    "After trying to find product traits as size and colors, it terned out that:\n",
    "\n",
    "color: 88.42% (461,350 in number) of the descriptions color column values are undefined.\n",
    "\n",
    "size: 86.85% (452,050 in number) of the descriptions color column values are undefined.\n",
    "\n",
    "So, delete those new columns from the data.. (but, using the lists later for the corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d8e27b",
   "metadata": {},
   "source": [
    "<b id=\"-Analysis\"> 2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e9e89d",
   "metadata": {},
   "source": [
    "## Product Description Analysis:\n",
    "\n",
    "   * Create a corpus out of product descriptions.\n",
    "   * Clean up the corpus and removed stopwords etc, Eliminate grammatical variations via stemming\n",
    "   * Create a Term-Frequency Inverse Document Frequency (TF-IDF) matrix.\n",
    "   * Clculate from the TF-IDF the corpus distance matrix comparing the relative similarity.\n",
    "   * Use the distance matrix to build a dendrogram from which the number of clusters will be detrmined \n",
    "   * Using Kmean to form the clusters\n",
    "   * Study the term frequencies for each cluster.\n",
    "   * Based on the term frequencies, identify product category keywords for each cluster.\n",
    "   * Categorizing the products in the store differently in order to get result that enable better understanding of product range. \n",
    "   * Calculating monthly revenue and monthly cumulating revenue to detect the trend\n",
    "   * Splitting products by category and finding: 1. the leading categories regarding the number of items in each category 2. the leading categories in sales\n",
    "   * Finding the top ten selling products\n",
    "   * Examining Refunds: by total amount and by frequency)\n",
    "      \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393b7189",
   "metadata": {},
   "source": [
    "<b id=\"-corpus\">    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d730df2",
   "metadata": {},
   "source": [
    "### Create a corpus out of product descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca503b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a copy in order to create a list of description for a corpus:\n",
    "\n",
    "description_df = df['description'].drop_duplicates().to_frame()\n",
    "display(description_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cecfc4a5",
   "metadata": {},
   "source": [
    "<b id=\"-clean\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9629c6",
   "metadata": {},
   "source": [
    "### Clean up the corpus and removed stopwords etc, Eliminate grammatical variations via stemming:\n",
    "\n",
    "Stopwords, stemming, and tokenizing:\n",
    "\n",
    "This section focused on defining some functions to manipulate the synopses. First, I load NLTK's list of English stop words. Stop words are words like \"a\", \"the\", or \"in\" which don't convey significant meaning. I'm sure there are much better explanations of this out there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c7686e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove non-words from corpus\n",
    "\n",
    "def remove_not_words(desc):\n",
    "    desc = re.sub(r'[^\\w]',' ', desc)\n",
    "    pattern = '[0-99]' \n",
    "    desc = re.sub(pattern,'', desc)\n",
    "    # delete single letters from description strings:\n",
    "    desc = ' '.join( [w for w in desc.split() if len(w)>1] )\n",
    "    return desc\n",
    "    \n",
    "description_df['filtered_corpus'] = description_df['description'].apply(remove_not_words)\n",
    "display(description_df)\n",
    "\n",
    "\n",
    "\n",
    "# Cleaning description list from stopwords\n",
    "stop_words = set(stopwords.words('english')) \n",
    "\n",
    "def remove_stop_words(desc):\n",
    "    word_tokens = word_tokenize(desc)\n",
    "    desc =  ' '.join([w for w in word_tokens if not w.lower() in stop_words])\n",
    "    return desc\n",
    "    \n",
    "description_df['filtered_corpus'] = description_df['filtered_corpus'].apply(remove_stop_words)\n",
    "display(description_df)\n",
    "\n",
    "\n",
    "def remove_size_color(desc):    \n",
    "    desc =  ' '.join([word for word in desc.split() if (word not in size_list and word not in colors_list )])\n",
    "    return desc\n",
    "\n",
    "description_df['filtered_corpus'] = description_df['filtered_corpus'].apply(remove_size_color)\n",
    "display(description_df)\n",
    "\n",
    "\n",
    "# lemmetizing the corpus in order to converts the word to its meaningful base form, \n",
    "# which is called Lemma . Thats allow us to get rid of plorals etc. \n",
    "\n",
    "wnl = nltk.WordNetLemmatizer()\n",
    "\n",
    "def lemmatize(desc):\n",
    "    desc = ' '.join([wnl.lemmatize(word) for word in nltk.wordpunct_tokenize(desc)])\n",
    "    return desc\n",
    "   \n",
    "description_df['filtered_corpus'] = description_df['filtered_corpus'].apply(lemmatize)\n",
    "display(description_df)    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b32f1b9",
   "metadata": {},
   "source": [
    "Conclusion:\n",
    "\n",
    "Now the corpus is ready for use. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f44a58",
   "metadata": {},
   "source": [
    "<b id=\"-tfidf\">    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d132adb",
   "metadata": {},
   "source": [
    "### Create a Term-Frequency Inverse Document Frequency (TF-IDF) matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba0d456",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Term-Frequency Inverse Document Frequency (TF-IDF) matrix \n",
    "\n",
    "final_corpus = list(description_df['filtered_corpus'])\n",
    "display(final_corpus)\n",
    "tfidf = TfidfVectorizer(stop_words='english',\n",
    "                                 use_idf=True, ngram_range=(1,1)) # stop_words='english'\n",
    "tfidf_matrix = tfidf.fit_transform(final_corpus)  # (corpus_list)\n",
    "\n",
    "print(tfidf_matrix.shape) \n",
    "\n",
    "display(len(final_corpus))\n",
    "\n",
    "terms = tfidf.get_feature_names() \n",
    "\n",
    "display(tfidf_matrix)\n",
    "\n",
    "display(terms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde67af2",
   "metadata": {},
   "source": [
    "<b id=\"-distance\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af69a913",
   "metadata": {},
   "source": [
    "### Calculate from the TF-IDF the corpus distance matrix comparing the relative similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097beb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# crating distance matrix : use tf idf for clustering:\n",
    "\n",
    "dist = pairwise_distances(tfidf_matrix, Y=None, metric = 'cosine')\n",
    "display(dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d288520",
   "metadata": {},
   "source": [
    "<b id=\"-dendrogram\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974df08b",
   "metadata": {},
   "source": [
    "### Use the distance matrix to build a dendrogram from which the number of clusters will be detrmined:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c0075e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# build a dendrogram from which the number of clusters will be detrmined\n",
    "linked = linkage(dist, method='ward') #  metric='correlation',method = 'ward',\n",
    "#The variable linked stores the table with the linked bundles of objects. It can be visualized as a dendrogram:\n",
    "plt.figure(figsize=(15, 10))  \n",
    "dendrogram(linked,  orientation=\"top\")# orientation='top',\n",
    "plt.title('Hierarchical Text clustering for Product Description')\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3457706f",
   "metadata": {},
   "source": [
    "<b id=\"-kmean\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8044e1",
   "metadata": {},
   "source": [
    "### Using Kmean to form the clusters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f8d910",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the k_means model with 5 clusters\n",
    "\n",
    "n_clusters = 5\n",
    "km =  KMeans(n_clusters = n_clusters, random_state = 1) # setting the number of clusters as 5\n",
    "# predict the clusters for observations (the algorithm assigns them a number from 0 to 2)\n",
    "labels = km.fit_predict(tfidf_matrix)  # applying the algorithm to the data and forming a cluster vector \n",
    "\n",
    "# store cluster labels in the field of our dataset\n",
    "\n",
    "description_df['cluster_km'] = labels\n",
    "#display(x_sc[1,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b358ba4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print()\n",
    "cluster_desc_grouped_df = description_df.groupby(['cluster_km'])['filtered_corpus'].unique().to_frame()\n",
    "print()\n",
    "\n",
    "cluster_desc_grouped_df.reset_index(inplace=True)\n",
    "#display(cluster_desc_grouped_df)\n",
    "\n",
    "# Now , to add the groups to event table :Let's create a dictionary with pair: user-group and then add it to events_clean:\n",
    "cluster_desc_dict = dict(zip(cluster_desc_grouped_df.cluster_km, cluster_desc_grouped_df.filtered_corpus))\n",
    "#display(cluster_desc_dict)\n",
    "\n",
    "\n",
    "for key in cluster_desc_dict.keys():\n",
    "   \n",
    "    print('cluster ', key, ' descriptions list:') # metal sign , christmas\n",
    "    print()\n",
    "    print(len(cluster_desc_dict[key]))\n",
    "    print(cluster_desc_dict[key])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4c7839",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in cluster_desc_dict[0]:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d5c930",
   "metadata": {},
   "source": [
    "<b id=\"-terms_freq\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1b1d7e",
   "metadata": {},
   "source": [
    "### Study the term frequencies for each cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39fbe30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we calculate the frequency of terms in each cluster to try to categorize the clusters\n",
    "\n",
    "cluster_term_frequency = 0*tfidf_matrix[:n_clusters,:]\n",
    "\n",
    "#display(tfidf_cluster_score.toarray())\n",
    "\n",
    "display(tfidf_matrix.shape[1])\n",
    "\n",
    "for row_index in range(tfidf_matrix.shape[0]):\n",
    "    cluster = labels[row_index]\n",
    "    cluster_term_frequency[cluster,:] += tfidf_matrix[row_index,:]>0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe35f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Study the terms frequencies for each cluster:\n",
    "  \n",
    "### Based on the term frequencies, identify product category keywords for each cluster.\n",
    "cluster_terms_dict = {}\n",
    "\n",
    "for cluster in range(cluster_term_frequency.shape[0]):\n",
    "    # create a list of values containing the terms belong to each clusrt (and locate it in the dict)\n",
    "    cluster_terms_list = []\n",
    "    # Sort each of the cluster's tfidf scores row:\n",
    "    cluster_term_frequency_sort_ind = np.argsort(cluster_term_frequency[cluster,:].toarray())\n",
    "    # Use the sorted items indices to sort the cluster term list:\n",
    "    for term_ind in reversed(cluster_term_frequency_sort_ind[0]):\n",
    "        # Choose only terms with tf > 0 (the term frequency is greater than zero):\n",
    "        if cluster_term_frequency[cluster,term_ind]>0:\n",
    "            # Add the term to an empty ranked term list \n",
    "            term = terms[term_ind]\n",
    "            cluster_terms_list.append(term) \n",
    "    # Join all term lists to a dictionary: \n",
    "    cluster_terms_dict[cluster] = cluster_terms_list\n",
    "        \n",
    "\n",
    "        \n",
    "# print the first value of dict:        \n",
    "for key in cluster_terms_dict.keys():\n",
    "    print('cluster ', key, ' keywords:')\n",
    "    print()\n",
    "    print(' '.join(cluster_terms_dict[key][:7]))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750ead98",
   "metadata": {},
   "source": [
    "<b id=\"-keywords\">    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc941eb0",
   "metadata": {},
   "source": [
    "### Based on the term frequencies, identify product category keywords for each cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36ebc69",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_top_terms_dict ={}    \n",
    "for key in cluster_terms_dict.keys():\n",
    "    cluster_top_terms_dict[key] = ' '.join(cluster_terms_dict[key][:10])\n",
    "    \n",
    "display(cluster_top_terms_dict)  \n",
    "\n",
    "description_df['cluster_keywords'] = description_df['cluster_km'].map(cluster_top_terms_dict)\n",
    "display(description_df.sample(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7529f0",
   "metadata": {},
   "source": [
    "Conclusion:\n",
    "\n",
    "The categories based on the clusterring characterized by keywords. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d9345d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_category_kp_dict ={}   \n",
    "\n",
    "for key in cluster_terms_dict.keys():\n",
    "    cluster_category_kp_dict[key] = ' '.join(cluster_terms_dict[key][:1])\n",
    "    \n",
    "display(cluster_category_kp_dict)  \n",
    "\n",
    "description_df['category_PK'] = description_df['cluster_km'].map(cluster_category_kp_dict)\n",
    "display(description_df.sample(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3983b417",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging the corpus back to the orginal data frame:\n",
    "df = pd.merge(df , description_df , on=['description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb5dcb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(list(df.filtered_corpus.unique()))\n",
    "display(len(list(df.filtered_corpus.unique())))# box, alarm cloc- 3524\n",
    "\n",
    "# df['item'] = df['filtered_corpus']\n",
    "# df.rename(columns = {'old_col1':'new_col1', 'old_col2':'new_col2'}, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d13f49",
   "metadata": {},
   "source": [
    "<b id=\"-categorization\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37452f20",
   "metadata": {},
   "source": [
    "### Categorizing the products in the store differently in order to get result that enable better understanding of product range:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0e2b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# description_df['category_PK'] = description_df['cluster_km'].map(cluster_category_kp_dict)\n",
    "# display(description_df.sample(10))\n",
    "m = Mystem()\n",
    "\n",
    "box_category_discriptors = ['box', 'container', 'bubble', 'recipe' ,'boombox','nesting', 'harmonica','cigar', 'snack', 'keepsake', 'sewing' , 'bucket']\n",
    "bags_category_discriptors = ['bag','washbag','shopper', 'luggage', 'purse', 'tidy', 'shoulder', 'recycling', 'storage', 'lunch']\n",
    "games_toys_categort_discriptors = [ 'game', 'domino' ,'dominoes', 'building' ,'blocks', 'doll', 'toy', 'helicopter', 'solider', 'catch', 'bingo', 'skipping', 'rope']\n",
    "\n",
    "#accessories_discriptors = ['hair grip', 'hair' , '']\n",
    "jewelry_category_discriptors = ['bracelet' ,'trinket','adornment','bauble','piercing', 'earrings','necklace', 'ring','grip', 'hair']\n",
    "school_categort_discriptors = ['eraser' ,'ruler', 'rubber', 'pen', 'pencil','notebook', 'scissor',  'chalk', 'crayon', 'calendar', 'calculator','chalkboard']\n",
    "dishes_categort_discriptors= ['cup' ,'teacup', 'mug',' moulds',' popcorn','cakestand','caddy' ,'plate', 'cake', 'tin', 'egg','cutter', 'tray','chopstick','bottle','dish','wine', 'bowl', 'trinket' ,'pot', 'cultery', 'jar', 'hot','bottle', 'food' , 'apron', 'picnic' ,'basket', 'lid', 'frying', 'pan', 'pot', 'oven' , 'tin', 'coaster' ]\n",
    "decoration_category_discriptors = ['decoration','candle','charm', 'garland', 'butterfiles','trellis', 'ornament', 'tree' ,'wall' , 'flag','christmas','santa', 'vase', 'bell', 'craft', 'feltcraft', 'flower','tattoo', 'star', 'place', 'letter', 'sign', 'canvas', 'incense', 'magnet','bank' ]\n",
    "bath_category_discriptors = ['towel' , 'bath', 'sponge', 'soap']\n",
    "furniture_category_discriptors = ['organiser','table' , 'cabinet', 'drawer', 'frame', 'picture', 'photo', 'frame', 'clock', 'mirror', 'knob', 'drawerknob', 'doormat', 'toadstool', 'stool', 'globe', 'doorstop', 'bin']\n",
    "garden_category_discriptors = ['herb' , 'brush', 'plant', 'ladder', 'peg',  'shed', 'bucket']\n",
    "party_category_discriptors = ['carnival','wrap','napkin','bunting', 'cone', 'baloon', 'ribbon', 'cloth' , 'card', 'birthday', 'party', 'invite', 'cupcake', 'stand' ,'gift', 'tape' , 'chain', 'disco', 'postage', 'sticker']\n",
    "holders_category_discriptors = ['holder', 'rack', 'hanger',  'hook']\n",
    "tools_category_discriptors =['scales', 'plasters','sqeezer', 'doorstop','aid', 'mallet', 'handsaw', 'hammer', 'plunger', 'saw', 'tool', 'toolbox', 'chisel', 'screw', 'screwdriver', 'wrench', 'nails', 'bolt', 'hacksaw', 'stepladder', 'pliers', 'drill']\n",
    "disposable_category_discriptors =['paper' ,'disposable']\n",
    "electronics_category_discriptors =['light','bulb', 'lamp', 'lantern' ,'torch','candlestick', 'candlelabrum', 'nightlight']\n",
    "bedding_clothes_category_discriptors = ['pillow', 'blanket', 'cushion', 'quilt', 'featerbed', 'bolster','curtain', 'pad', 'skirt' , 'dress','hat', 'slipper', 'poncho']#pad?\n",
    "\n",
    "# 17 categories\n",
    "category_descriptors = {\n",
    "    'disposable': disposable_category_discriptors,\n",
    "    'dishes': dishes_categort_discriptors,\n",
    "    'holders/hangers' : holders_category_discriptors,\n",
    "    'boxes' : box_category_discriptors,\n",
    "    'bags' : bags_category_discriptors,\n",
    "    'electronics':electronics_category_discriptors,\n",
    "    'games': games_toys_categort_discriptors,\n",
    "    'decoration': decoration_category_discriptors,\n",
    "    'jewelry': jewelry_category_discriptors,\n",
    "    'party': party_category_discriptors,\n",
    "    'garden': garden_category_discriptors,\n",
    "    'school': school_categort_discriptors,\n",
    "    'furniture': furniture_category_discriptors, \n",
    "    'bath': bath_category_discriptors,\n",
    "    'tools': tools_category_discriptors,\n",
    "    'clothes/bedding':bedding_clothes_category_discriptors\n",
    "}\n",
    "\n",
    "\n",
    "def find_category_func(filtered_string_descirption):\n",
    "    #lemmatized = m.lemmatize(user_string_descirption)\n",
    "    #lemmatized = [word.lower() for word in lemmatized]\n",
    "    for category, descriptor in category_descriptors.items():       \n",
    "        if any((word in filtered_string_descirption) for word in descriptor):\n",
    "            return category\n",
    "    return 'other'\n",
    "\n",
    "df['category'] = df['filtered_corpus'].apply(find_category_func)\n",
    "\n",
    "#display(df[df['category']=='others'])# none\n",
    "display((df[df['category']=='boxes']['description']).unique())# curtain, book\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9c2d19",
   "metadata": {},
   "source": [
    "Conclusion:\n",
    "\n",
    "Categorizing the products (using the corpus made for Clustering) enables better analysis of the product range. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75eb8f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = df.columns.str.replace('filtered_corpus','filtered_corpus')\n",
    "print('num of stock_code')\n",
    "display(df['stock_code'].nunique()) # 3910 rows now\n",
    "print()\n",
    "print('num of descriptions')\n",
    "display(df['description'].nunique()) # 4018\n",
    "print()\n",
    "print('num of filtered_corpus')\n",
    "print(df['filtered_corpus'].nunique())# 3379\n",
    "print()\n",
    "print('num of invoice_no') # 19780\n",
    "print(df['invoice_no'].nunique())\n",
    "print()\n",
    "print('num of rows') # 520520\n",
    "print(df.shape[0])\n",
    "print()\n",
    "\n",
    "\n",
    "df.rename(columns={\"filtered_corpus\": \"item\"}, inplace = True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3dbf0db",
   "metadata": {},
   "source": [
    "<b id=\"-revenue\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988974c8",
   "metadata": {},
   "source": [
    "### Calculating monthly revenue and monthly cumulating revenue to detect the trend: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e3a92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rev_df = df.groupby(['invoice_period'])['tot_order'].sum().reset_index()\n",
    "display(rev_df)\n",
    "\n",
    "plot_data = [\n",
    "    go.Scatter(\n",
    "        x=rev_df['invoice_period'],\n",
    "        y=rev_df['tot_order'],\n",
    "        mode='lines+markers'\n",
    "    )\n",
    "]\n",
    "plot_layout = go.Layout(\n",
    "        xaxis={\"type\": \"category\"},\n",
    "        title='Monthly Revenue'\n",
    ")\n",
    "\n",
    "fig = go.Figure(data=plot_data, layout=plot_layout)\n",
    "pyoff.iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5bd1945",
   "metadata": {},
   "source": [
    "Conclusion:\n",
    "\n",
    "The number of orders started increasing from the very begining , yet it is becoming more dramatic increase from August 2019  and the maximum number of orders are in November 2019!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cac1902",
   "metadata": {},
   "outputs": [],
   "source": [
    "invoice_period_grouped = df.groupby('invoice_period')\n",
    "invoice_period_agg = invoice_period_grouped.agg({'description': pd.Series.nunique,\n",
    "                      'quantity': np.sum,\n",
    "                      'tot_order': np.sum})\n",
    "invoice_period_agg.rename(columns ={ 'description':'total_product_types',\n",
    "                                     'quantity':'total_units',\n",
    "                                      'tot_order': 'total_order_rev'}, inplace = True)\n",
    "invoice_period_agg['total_order_rev']= round(invoice_period_agg['total_order_rev'],1)\n",
    "\n",
    "display(invoice_period_agg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a877f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = invoice_period_agg['total_order_rev'].cumsum().plot()\n",
    "ax.set_yscale('linear')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f04729d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax2 = invoice_period_agg['total_order_rev'].cumsum().plot()\n",
    "ax2.set_yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642a61cd",
   "metadata": {},
   "source": [
    "Conclusion:\n",
    "\n",
    "The general trend is clearly a positive one during the all period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b92ddd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort the table by each of the columns:\n",
    "print('Sorted by number of total units in each month:')\n",
    "sorted_by_units_invoice_period_agg = invoice_period_agg.sort_values(by='total_units',  ascending=False)\n",
    "display(sorted_by_units_invoice_period_agg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8738aa10",
   "metadata": {},
   "source": [
    "Conclusion:\n",
    "\n",
    "The invoice period with the highest units sold is 2019-11. \t769207 units were sold. (out of 2907 product types that were sold then and generated 1480865.9 for revenur rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08566af",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Sorted by total revenue in each month:')\n",
    "sorted_by_revs_invoice_period_agg = invoice_period_agg.sort_values(by='total_order_rev' ,  ascending=False)\n",
    "display(sorted_by_revs_invoice_period_agg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7583bff3",
   "metadata": {},
   "source": [
    "Conclusion:\n",
    "\n",
    "The invoice period with the highest revenue(for orders) sold is also 2019-11. Revenue of  1480865.9 was generated then."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f58520",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Sorted by total product type in each month:')\n",
    "sorted_by_product_type_invoice_period_agg = invoice_period_agg.sort_values(by='total_product_types',  ascending=False)\n",
    "display(sorted_by_product_type_invoice_period_agg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd1ef54",
   "metadata": {},
   "source": [
    "Conclusion:\n",
    "\n",
    "The invoice period with the largest range of product sold is also 2019-11. 769207 units were sold, out of 2907 product types that were sold then and generated 1480865.9 for revenur rate)\n",
    "\n",
    "Make sense if the if assortment have not change dramatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c090c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_value_label(x_list,y1_list, y2_list):\n",
    "    for i in range(0, len(x_list)):\n",
    "        plt.annotate(y1_list[i],(i,y1_list[i]),ha=\"center\")\n",
    "        plt.annotate(y2_list[i],(i,y2_list[i]),ha=\"center\")\n",
    "\n",
    "\n",
    "invoice_period_agg.reset_index(inplace=True)\n",
    "invoice_period_agg.plot(x=\"invoice_period\", y=[\"total_units\", \"total_order_rev\"], kind=\"bar\", color=[\"C4\",\"C9\"], title='Units and Revenue per Month', figsize= (18,14))\n",
    "add_value_label(invoice_period_agg.invoice_period, invoice_period_agg.total_order_rev, invoice_period_agg.total_units,)\n",
    "display(invoice_period_agg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37348f1d",
   "metadata": {},
   "source": [
    "Conclusion:\n",
    "\n",
    "\n",
    "The table and the bar above shows how November 2019 has the best sales considering all of three: units, range of product type and amount of generated profit (revenue) that were sold. \n",
    "\n",
    "The lowest goes to November 2018! interesting what is the reason to that conspicuous dramatic change.. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c347ac5",
   "metadata": {},
   "source": [
    "<b id=\"-split\">     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee60f03",
   "metadata": {},
   "source": [
    "### Splitting products by category and finding: \n",
    " - The leading categories regarding the number of items in each category \n",
    " - The leading categories in sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67f771a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting products by category and finding the leading categories regarding the number of items in each category:\n",
    "\n",
    "# Proportions of the various types of product\n",
    "# leading product - most selling :\n",
    "\n",
    "grouped_items=df.groupby(['category'])['description'].nunique().reset_index()\n",
    "grouped_items.columns = ['item_category' , 'max_number_items']\n",
    "display(grouped_items)\n",
    "\n",
    "fig3 = px.pie(grouped_items, values=grouped_items.max_number_items, names=grouped_items.item_category,\n",
    "              color=grouped_items.item_category,\n",
    "color_discrete_map={'Sendo':'cyan', 'Tiki':'royalblue','Shopee':'darkblue'})\n",
    "fig3.update_layout(\n",
    "title=\"<b> Split by Categories</b>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53cca783",
   "metadata": {},
   "source": [
    "Conclusion:\n",
    "\n",
    "The leading product category in number of items is dishes  (24.3%) with decoration coming second , follwed by decorations (19.7%). \n",
    "In other words, the category with the largest range of product is dishes! then decoration..\n",
    "\n",
    "The category with the smallest range of product is tools with 13 items only!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55121f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting products by category and finding the leading categories in sales:\n",
    "\n",
    "df_item_sales=df.groupby([\"category\"])['tot_order'].sum().sort_values(ascending=False)\n",
    "df_item_sales=df_item_sales.to_frame()\n",
    "# Reset the index of dataframe\n",
    "modified = df_item_sales.reset_index()\n",
    "modified.rename(columns={'tot_order':'sales'}, inplace=True)\n",
    "\n",
    "from matplotlib import pyplot\n",
    "a4_dims = (11.7, 8.27)\n",
    "fig, ax = plt.subplots(figsize=a4_dims)\n",
    "sns.barplot(x = \"sales\",\n",
    "            y = \"category\",\n",
    "            data = modified)\n",
    "plt.title(\"Category wise Sales\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10fc1e46",
   "metadata": {},
   "source": [
    "Conclusion:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2403e5f",
   "metadata": {},
   "source": [
    "The leading product category in sales is also dishes with decoration coming second again and tools again last."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4033c6e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "083efd1e",
   "metadata": {},
   "source": [
    "<b id=\"-selling\">    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b225e9",
   "metadata": {},
   "source": [
    "### Finding the top ten selling products:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b73080",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#import missingno as msno\n",
    "from textwrap import wrap\n",
    "# top selling items by sales\n",
    "sales_order = df.groupby('description').sum()['tot_order'].nlargest(10)#description\n",
    "\n",
    "plt.figure(figsize = (30,10))\n",
    "ax = sns.barplot(x = sales_order.index, y = sales_order.values, palette = 'seismic_r')\n",
    "ax.set_xlabel('Product Description', fontsize = 20)\n",
    "ax.set_ylabel('Total Sales', fontsize = 20)\n",
    "ax.set_title('Top 10 Selling Products', fontsize = 30)\n",
    "\n",
    "labels = [ '\\n'.join(wrap(l, 15)) for l in sales_order.index ]\n",
    "ax.set_xticklabels(labels, fontsize = 15)\n",
    "\n",
    "value_labels = []\n",
    "for x in sales_order.values:\n",
    "    value_labels.append(str(int(x/1000))+' k')\n",
    "\n",
    "for p, label in zip(ax.patches, value_labels):\n",
    "    ax.annotate(label, (p.get_x() + 0.26, p.get_height() + 2), fontsize = 15)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ba56c8",
   "metadata": {},
   "source": [
    "Conclusion:\n",
    "\n",
    "The leading product in total sales  is cake-stand with paper little birdie coming second and hanging heart t-light holder coming third in sales ..\n",
    "\n",
    "Jumbo bag, storage jar, rabbit night light , paper chain kit 50's christmas, assorted collo bird ornament, chilly light and spotty bounting are in the top ten as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af66f1c",
   "metadata": {},
   "source": [
    "<b id=\"-refunds\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db64017e",
   "metadata": {},
   "source": [
    "###  Examining Refunds: by total amount and by frequency:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce967f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examining Refunds by total amount:  \n",
    "# Refunds/Cancellations:\n",
    "# Lets take a look at refunds and what the most frequent and largest refunds are.\n",
    "\n",
    "refund_df = pd.concat([neg_zero_unit_price_df, refunds_df_cancelations])\n",
    "\n",
    "plt.figure(figsize = (40,10))\n",
    "ax = sns.barplot(x = refund_df['description'].value_counts().nlargest(10).index, y = refund_df['description'].value_counts().nlargest(10).values, palette = 'seismic_r')\n",
    "ax.set_xlabel('Item Description', fontsize = 20)\n",
    "ax.set_ylabel('Number of Refunds', fontsize = 20)\n",
    "ax.set_title('Top 10 Refunded Items by Frequency', fontsize = 30)\n",
    "\n",
    "labels = [ '\\n'.join(wrap(l, 20)) for l in refund_df['description'].value_counts().nlargest(10).index ]\n",
    "ax.set_xticklabels(labels, fontsize = 15)\n",
    "\n",
    "for p, label in zip(ax.patches, refund_df['description'].value_counts().nlargest(10).values):\n",
    "    ax.annotate(label, (p.get_x() + 0.3, p.get_height() + 1), fontsize = 20)\n",
    "\n",
    "top_refund_amt = -refund_df.groupby('description').sum()['tot_order'].nsmallest(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb119ce",
   "metadata": {},
   "source": [
    "Conclusion:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f8d92e",
   "metadata": {},
   "source": [
    "Surprisingly, Cake-stand also stars in the first place amont refunded items by frequency , followed by postage and jam making set with jars. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69594d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examining Refunds by total amount and by frequency:  \n",
    "plt.figure(figsize = (40,10))\n",
    "ax = sns.barplot(x = top_refund_amt.index, y = top_refund_amt.values, palette = 'seismic_r')\n",
    "ax.set_xlabel('Item Description', fontsize = 20)\n",
    "ax.set_ylabel('Total Amount in Refunds', fontsize = 20)\n",
    "ax.set_title('Top 10 Refunded Items by Total Amount', fontsize = 30)\n",
    "\n",
    "labels = [ '\\n'.join(wrap(l, 20)) for l in top_refund_amt.index ]\n",
    "ax.set_xticklabels(labels, fontsize = 15)\n",
    "\n",
    "for p, label in zip(ax.patches, top_refund_amt.values):\n",
    "    ax.annotate(label, (p.get_x() + 0.2, p.get_height() + 10), fontsize = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e304bf",
   "metadata": {},
   "source": [
    "Conclusions:\n",
    "\n",
    "Paper craft, little birdie is in the first place amont refunded items by total amount. Postage moved to the third place here while medium ceramic top storage jar is in the second place instead!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30320aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "large_refunds = refund_df['tot_order'].nsmallest(10).index\n",
    "for refund in large_refunds:\n",
    "    print(-refund_df.loc[refund]['quantity'], ' units of ', refund_df.loc[refund]['description'])#, ' refunded at ', -refund_df.loc[refund]['tot_order'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46df5b6e",
   "metadata": {},
   "source": [
    "conclusion:\n",
    "\n",
    "Above are the largest refunds products with their names and number of units.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cbb966d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding \n",
    "#df['month'] = df['invoice_date'].dt.month_name()\n",
    "#df['year'] = df['invoice_date'].dt.year\n",
    "\n",
    "#df['month_year'] = pd.to_datetime(df['birth_date']).dt.to_period('M')\n",
    "#df['month'] = pd.DatetimeIndex(df['invoice_date']).month\n",
    "#display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40510e0",
   "metadata": {},
   "source": [
    "<b id=\"-recommender\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b722615e",
   "metadata": {},
   "source": [
    "\n",
    "## Product Bundle(Basket) Analysis and Recommender System:\n",
    "\n",
    "   * Create a basket for each transaction (invoice) and study the popullarity for products to appear (in any transaction/invoice) as well as for their different combinations by using Apriori Algorithm\n",
    "   * Calculate other Basket metrics using association_rule method in order to realize how to expand sails in the basket level by recommending additional products\n",
    "   * Studying product similarities in order to realize how to mantain and increase profits (in a basket level) by recommanding sustitutional/interchangable products    \n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a76765b",
   "metadata": {},
   "source": [
    "<b id=\"-recommender\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538ee953",
   "metadata": {},
   "source": [
    "**Support** - popularity of the item overall.To calculate support we need to take all transactions that included item, count them and then divide by the total number of transactions (popularity in the basket).\n",
    "\n",
    "**Condfidence** - is the probability that a item 2 will be bought together with item 1. <br>\n",
    "To calculate confidence we take all the transactions when to items were sold together and then divide their number by the number of transactions that had item 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d7eb3b",
   "metadata": {},
   "source": [
    "basket - purchase wise get rules of what items are boought together,, what  popular items, what items increase the chances of other items been bought  , or general what to reccomand base on what is already boght (appriori and assosiate. statistic relationships of items) recommed what  recommend base on similarity of what you bought. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506cf235",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many transactions - purchases we have: 19780\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a089bd",
   "metadata": {},
   "source": [
    "<b id=\"-basket\">  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e043f8",
   "metadata": {},
   "source": [
    "### Create a basket for each transaction (invoice) and study the popullarity for products to appear (in any transaction/invoice) as well as for their different combinations by using Apriori Algorithm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe970fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group data by transaction and item to calculate how many transactions had our item of interest. creating a basket to each transaction.\n",
    "purchases_df = df.groupby(['invoice_no', 'item'])['item'].count().reset_index(name ='count')\n",
    "# in order to realize for any purchase/transaction (invoice_no)  count how many items were bought in each order.\n",
    "#this is a general df, we will use it for the algorythm itself as well. \n",
    "\n",
    "display(purchases_df)\n",
    "print()\n",
    "display()\n",
    "purchases_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca17f15",
   "metadata": {},
   "source": [
    "Conclusion:\n",
    "\n",
    "There are 489668 rows meaning there are 489668 purchases!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56ef7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First of all we need to create a basket for each transaction/ purchase.\n",
    "\n",
    "basket_df = (purchases_df\n",
    "          .groupby(['invoice_no', 'item'])['count']\n",
    "          .sum().unstack().reset_index().fillna(0)\n",
    "          .set_index('invoice_no'))\n",
    "\n",
    "display(basket_df.sample(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa0ab0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since it doesn't matter to us how many items were bought, just the fact that the item was bought in the transactions, use the following \n",
    "# function to turn numerical to categorical(1 or 0. exist in basket or not. to calculate probab..)\n",
    "\n",
    "def encode_units(x):\n",
    "    if x <= 0:\n",
    "        return 0\n",
    "    if x >= 1:\n",
    "        return 1\n",
    "\n",
    "basket_sets = basket_df.applymap(encode_units)# 19780 \n",
    "display(basket_sets)# all optional item sets. for later..comb for appriori use"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8401b6c1",
   "metadata": {},
   "source": [
    "Now , this basket can be passed to apriori algorithm that can calculate support for all items and their combinations.\n",
    "This algorithm requires to define min_support which is the minimum support that we consider. \n",
    "Let's define low min_support which means that only items and combinations that appeared at least 3% of a time will be included.\n",
    "    \n",
    " **Note**: min_support in apriori will later influence our other calculations, it's ok to set lower min_support, but to have more combinations later    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9deda2bd",
   "metadata": {},
   "source": [
    "Appriori returns items and pairs of items sorted by support .\n",
    "\n",
    "Appriori calculates 'support' values for all product and their combinations. (all possible combinations (baskets sets) in a basket)\n",
    "\n",
    "analayzing basket look in every purchase..(weather we have x in basket-purchases or not. (and not how many..) \n",
    "Thus, Appriori help to find the (strong) and weak links then use Association Rule to uplift sales "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb985a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "frequent_itemsets = apriori(basket_sets, min_support=0.03, use_colnames=True)\n",
    "frequent_itemsets.sort_values(by='support',ascending=False)\n",
    "# min_support of 3% means that item appeared on atleast 3% of buskets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a467ee2f",
   "metadata": {},
   "source": [
    "Appriori returns items and pairs of items sorted by support \n",
    "\n",
    "Appriori calculates 'support' values for all product and their combinations. (all possible combinations (baskets sets) in a basket)\n",
    "\n",
    "analayzing basket look in every purchase..(weather we have x in basket-purchases or not. (and not how many..) \n",
    "Thus, Appriori help to find the (strong) and weak links then use Association Rule to uplift sales "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34dd66c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# support for x: (subsetting)count number of purchases that has x in it then devide it by num of total purchases there are\n",
    "# the result of suport is the share of baskets with the item.\n",
    "\n",
    "print('The top leading products regarding support:')\n",
    "display(frequent_itemsets.sort_values(by='support', ascending=False).head(20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d886ede9",
   "metadata": {},
   "source": [
    "Conclusion:\n",
    "\n",
    "Hanging heart holder appears in 12.7% of the baskets, heart wicker appears in 10.8%, bag retrospot in 10.5% and regency cakestand tier with 10% etc..\n",
    "\n",
    "Those are the 20 leading popular products (support wize): \n",
    "\n",
    "hanging heart holder, heart wicker, bag retrospot, regency cakestand tier, alarm clock bakelike, lunch bag retrospot, assorted bird ornament, popcorn holder, cake tin pantry design, pack retrospot cake case, lunch bag suki design, lunch bag skull,natural slate heart chalkboard, bag polkadot, storage bag suki, shopper vintage paisley, jam making printed, paper chain kit christmas, lunch bag spaceboy design, lunch bag car.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b324d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The least  popular products regarding support:')\n",
    "\n",
    "# Creating a list of unpopular items (regarding 'support') in order to treat them later on..\n",
    "unpopular_product_list = ['pencil tube skull', 'bag pear', 'doormat new england', 'childrens cutlery polkadot', 'pack dinosaur cake case', 'hanging heart zinc holder', 'edwardian parasol', 'lunch bag retrospot', 'lunch bag polkadot', 'vintage head tail card game', 'hot bath metal sign',' traditional knitting nancy' ,'wicker star', 'paper chain kit retrospot', 'vintage christmas napkin', 'stripe ceramic drawer knob',\n",
    "'bread bin diner style', 'sweetheart fairy cake case', 'doormat union flag', 'cream felt craft trinket box' , 'cake case vintage christmas']\n",
    "\n",
    "display(frequent_itemsets.sort_values(by='support',ascending=True).head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c0faf1",
   "metadata": {},
   "source": [
    "Conclusion:\n",
    "\n",
    "Those are the 20 least leading ones: \n",
    "\n",
    "pencil tube skull, bag pear, doormat new england, childrens cutlery polkadot, pack dinosaur cake case, hanging heart zinc holder, edwardian parasol, lunch bag retrospot, lunch bag polkadot, vintage head tail card game, hot bath metal sign, traditional knitting nancy ,wicker star, paper chain kit retrospot, vintage christmas napkin, stripe ceramic drawer knob,\n",
    "bread bin diner style, sweetheart fairy cake case, doormat union flag, cream felt craft trinket box and cake case vintage christmas.\n",
    "\n",
    "All of them with popularity rate('support' value) of 3% in baskets. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ae493b",
   "metadata": {},
   "source": [
    "<b id=\"-additional\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6c9a5b",
   "metadata": {},
   "source": [
    "### Calculate other Basket metrics using association_rule method in order to realize how to expand sails in the basket level by recommending additional products:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90607c48",
   "metadata": {},
   "source": [
    " **Lift** - in what ratio is the purchase of items bought together is more likely than separately?\n",
    "\n",
    "General rule of thumb for lift:\n",
    "- If lift>1 than products have high chance to be bough together and \"uplift\" each other <br> \n",
    "- If lift=1 than products don't really influence each other <br> \n",
    "- If lift<1 than products there is no connection between products<br> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30feef9a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# calculating it to combination of items and their lift - to find the top comb which lift each other up..\n",
    "# sort by confidence in descending order:\n",
    "rules = association_rules(frequent_itemsets, metric = \"lift\", min_threshold = 1)#48 sec\n",
    "rules.sort_values('confidence', ascending = False, inplace = True)\n",
    "display(rules.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2edc491",
   "metadata": {},
   "source": [
    "Here we see on top our \"pair\" Toast and Coffee, and their metrics. Let's take a look at them one more time and get to know some new metrics: <br>\n",
    "- **antecedents** - means the \"if\" part of the sentence \"IF I buy 'roses regency teacup and saucer , pink regenc.'(Toast),...\" <br>\n",
    "- **consequents** -  \"then\" part of the sentence \"...,then the probability of buying 'green regency teacup and saucer'(Coffee...\"- is confidence(90%\n",
    "- **antecedent/consequent support** -  corresponding supports of the items. Do not change depending on the pair.\n",
    "- **leverage** - another measure of independence. leverage=0 equals 0 independence\n",
    "- **conviction** - Similar as lift, but means that consequent is highly depending on the antecedent. Rules of thumb the same as for lift"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb06c38e",
   "metadata": {},
   "source": [
    "Conclusions :\n",
    "\n",
    "The highest confidence  77.4% of the baskets goes to: 'rose regency teacup saucer'\tand 'regency teacup saucer'. meaning that if a customer buys a rose regency teacup saucer 77.4% that he'll also buy regency teacup saucer. \n",
    "Their total support is 4.1% , antecedent support (of the first) is 5.3% ,consequent support (of the last) is 5.8%. \n",
    "This pair has the highest lift (13.3%) meaning that buying the first uplift the prop of buying the other. \n",
    "  \n",
    "Another pair with  pretty high confidence of 67.7% of the baskets is: 'bag polkadot' and 'storage bag suki'. meaning that if a customer buys a bag polkadot 67.7% that he'll also buy regency storage bag suki. \n",
    "Their total support is 4.1% , antecedent support (of the first) is 6.1% ,consequent support (of the last) is 10.5%. \n",
    "This pair also has the high lift (6.4%) meaning that buying the first uplift the prop of buying the other. \n",
    "Lift is the ratio that the products are bought together more likely than seperately. (Lift(x+y) = (Confidence (x+y))/(Support (x)))\n",
    "\n",
    "Note : The order may change the reasult: for example 'support same, 'confidece' values chage when changing the order of items.\n",
    "Yet the overall support does not change and nither does the lift (since the setting here is to keep the higher in this case)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8976d932",
   "metadata": {},
   "source": [
    "\n",
    " meaning:if i buy y how purchase of it uplift the prop of buying of x\n",
    " That means that the ratio of increase of Coffe sales if Toast is bought is  0.1, which means that it doesn't matter to \"Coffee\" whether it's sold alone or together with toast\n",
    "\n",
    "  conf_coffee_toast/sup_cofee. if low it wouldnt uplift coffee\n",
    " That means that the ratio of increase of Coffe sales if Toast is bought is  0.1, which means that it doesn't matter to \"Coffee\" wheth        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a35f39",
   "metadata": {},
   "source": [
    "Association Rules vs Recommender System: ( here, Similarity System specificly)  \n",
    "\n",
    "While the first allow expanding the basket and understand how to increse sales of an item knowing the item that already exist in the basket, the last detect similar items (in the basket) which \"behaves\" the same way , thus maximizing profit. \n",
    "\n",
    "The  first can be used for suggesting additional items while the last can be used for applying the same marketing strategies to similar items in the basket or suggesting simmilar items whem a specific wanted item is missing (the business is ruuning out of it)  \n",
    "\n",
    "In other words, adding additional items vs interchangable items in basket. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f83d386",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "216c24d4",
   "metadata": {},
   "source": [
    " we can reccommend 'green regency teacup and saucer' wheever we sell :roses regency teacup and saucer , pink regenc..., green regency teacup and saucer, pink regency.., pink regency teacup and saucer). leverage not high -do not influence the overall no of transactions. you can look vice versa . (support same, confidece - changes, lift - same cause they keep the highlift, conviction -1 - no rela"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f69ab4c",
   "metadata": {},
   "source": [
    "<b id=\"-interchangable\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c02ee4",
   "metadata": {},
   "source": [
    "### Studying product similarities in order to realize how to mantain and increase profits (in a basket level) by recommanding sustitutional/interchangable products:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eac1001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# recommendder system-Similiarity: cosine similary(there aaaere diffrent kinds than that.)\n",
    "\n",
    "basket_new_df=basket_df\n",
    "basket_new_df = basket_new_df.fillna(0).reset_index()\n",
    "basket_new_df=basket_new_df.drop('invoice_no', axis=1)\n",
    "basket_new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d916ffc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "co_matrix = basket_new_df.T.dot(basket_new_df)\n",
    "np.fill_diagonal(co_matrix.values, 0)\n",
    "display(co_matrix)#looking for same pattern. complementary items. give back items behave "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1bfc417",
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_score_df = pd.DataFrame(cosine_similarity(co_matrix))\n",
    "cos_score_df.index = co_matrix.index\n",
    "cos_score_df.columns = np.array(co_matrix.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6697112c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_score_df=cos_score_df.reset_index()\n",
    "display(cos_score_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e041e7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating recommender system: items that behave the same as.. \n",
    "\n",
    "display(cos_score_df[cos_score_df.item=='cake case vintage christmas'].T)\n",
    "\n",
    "def find_top_in(check):\n",
    "    subset=cos_score_df[cos_score_df.filtered_corpus==check]\n",
    "    subset=subset.T.reset_index()\n",
    "    subset.columns=['pairs','correlation']\n",
    "    #removing item itself from dataset\n",
    "    subset=subset[subset.correlation!=check]\n",
    "    subset=subset[subset.pairs!=check]\n",
    "    subset_top=subset.sort_values(by='correlation',ascending=False).head(10)\n",
    "    return subset_top\n",
    "\n",
    "\n",
    "#for product in df['description'].values:\n",
    "#    find_top_in(product)\n",
    "# pencil tube skull, bag pear, doormat new england, childrens cutlery polkadot, pack dinosaur cake case, hanging heart zinc holder, edwardian parasol, lunch bag retrospot, lunch bag polkadot, vintage head tail card game, hot bath metal sign, traditional knitting nancy ,wicker star, paper chain kit retrospot, vintage christmas napkin, stripe ceramic drawer knob,\n",
    "# bread bin diner style, sweetheart fairy cake case, doormat union flag, cream felt craft trinket box and cake case vintage christmas.\n",
    "\n",
    "#All of them with popularity rate('support' value) of 3% in baskets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f98739f",
   "metadata": {},
   "source": [
    "Conclusion:\n",
    "\n",
    "The online store can use this function to find the top 10 products that resamble items which are missing or to imply marketing strategy on them.. (Perhaps for some of the product with the lowest support or the ones we consider unpopular for other reasons)\n",
    "\n",
    "For example one of the product with the lowest support is cake case vintage christmas when applying the function on this it returns product product with similar behavior. 'abc treasure book box' is the one that resamble it the  most. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4937033",
   "metadata": {},
   "source": [
    "<b id=\"-market\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e1408f",
   "metadata": {},
   "source": [
    "## Product Market Values Analysis:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3b187e",
   "metadata": {},
   "source": [
    "   * Calculate product recency-frequency-monetary (RFM Metrics)\n",
    "   * Study product RFM distributions (in order to determine the segmentation split\n",
    "   * Categorize products based on RFM frame\n",
    "   * Charactarize the product clusters based on R, F and M scores   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e57327a",
   "metadata": {},
   "source": [
    "<b id=\"-market\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ba5cf6",
   "metadata": {},
   "source": [
    "<b id=\"-rfm\">    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883a559a",
   "metadata": {},
   "source": [
    "### Calculate product recency-frequency-monetary (RFM Metrics):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef1deb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine recency:\n",
    "from datetime import date\n",
    "today=date.today()\n",
    "display(today)\n",
    "\n",
    "\n",
    "import datetime as dt\n",
    "NOW=dt.datetime(2019,12,8)# 2018-11-29   2019-12-07"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18159447",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating recency, frequency and monetary for each item (using the filtered corpus):\n",
    "rfm=df.groupby('item').agg({\n",
    "    'invoice_date': lambda x: (NOW-x.max()).days, # last date of purchase for product. max date for product \n",
    "    'stock_code': 'count', #nuinque #invoice_no\n",
    "    'unit_price':'sum'\n",
    "}).reset_index()\n",
    "\n",
    "rfm.rename(columns=\n",
    "                    {'invoice_date':'recency',\n",
    "                    'stock_code':'frequency',\n",
    "                   'unit_price':'monetary'},inplace=True)\n",
    "\n",
    "\n",
    "display(rfm.describe())\n",
    "display(rfm.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d8f5fd",
   "metadata": {},
   "source": [
    "Conclusion:\n",
    "\n",
    "There  3374 4ows and 4 columns. no missing values or duplicates\n",
    "\n",
    "description-product\trecency\tfrequency\tmonetary_value\n",
    "recency - how many days passed since the last purchase. min:1, max: 374, mean:47.16.\n",
    "frequency - how often the product is bought. min:1,  max: 3865, mean:154.27.\n",
    "monetary -  how much money the product produced. min : 0.004, max: 28091, mean: 506.60. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2ec9f6",
   "metadata": {},
   "source": [
    "<b id=\"-split\">   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28323eb",
   "metadata": {},
   "source": [
    "### Study product RFM distributions (in order to determine the segmentation split):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3936b286",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to check skewness of rfm df:\n",
    "def check_skew(rfm_df, column):\n",
    "    skew = stats.skew(rfm_df[column])\n",
    "    skewtest = stats.skewtest(rfm_df[column])\n",
    "    plt.title('Distribution of ' + column)\n",
    "    sns.distplot(rfm_df[column])\n",
    "    sns.set(rc={'figure.figsize':(12.7,6.27)} )\n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "\n",
    "check_skew(rfm ,'recency')\n",
    "check_skew(rfm,'frequency')\n",
    "check_skew(rfm,'monetary')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5076aac4",
   "metadata": {},
   "source": [
    "Conclusion: The recency, frequency and monatery are highly skewed, so we will have to log-scale them in order to get a good segmentation split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f36eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing skewness by log-scaling\n",
    "rfm_log = rfm.copy()\n",
    "rfm_log['recency'] = np.log(rfm_log['recency']+1)\n",
    "rfm_log['frequency'] = np.log(rfm_log['frequency']+1)\n",
    "rfm_log['monetary'] = np.log(rfm_log['monetary']+1)\n",
    "\n",
    "plt.figure(figsize=(9, 9))\n",
    "plt.subplot(3, 1, 1)\n",
    "check_skew(rfm_log,'recency')\n",
    "plt.subplot(3, 1, 2)\n",
    "check_skew(rfm_log,'frequency')\n",
    "plt.subplot(3, 1, 3)\n",
    "check_skew(rfm_log,'monetary')\n",
    "plt.tight_layout()\n",
    "\n",
    "rfm.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42bb9246",
   "metadata": {},
   "source": [
    "Conclusions:\n",
    "After log-scaling we are ready for segmentation. We will choose K-means to find good segmentation of the recency, frequency and monetery. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957ff9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first step for k-means is to fit-transform the 'recency','frequency' and 'monetary' distributions: \n",
    "\n",
    "rfm_log_scaled = rfm_log.copy()\n",
    "scaler = StandardScaler()\n",
    "trans_array = scaler.fit_transform(rfm_log_scaled[['recency','frequency','monetary']])\n",
    "rfm_log_scaled[['recency','frequency','monetary']] = trans_array\n",
    "\n",
    "display(rfm_log_scaled)\n",
    "\n",
    "plt.figure(figsize=(9, 9))\n",
    "plt.subplot(3, 1, 1)\n",
    "check_skew(rfm_log_scaled,'recency')\n",
    "plt.subplot(3, 1, 2)\n",
    "check_skew(rfm_log_scaled,'frequency')\n",
    "plt.subplot(3, 1, 3)\n",
    "check_skew(rfm_log_scaled,'monetary')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd118d07",
   "metadata": {},
   "source": [
    "<b id=\"-categorize\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673611d0",
   "metadata": {},
   "source": [
    "## Categorize products based on K-MEANS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7cc3b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let's see how many clusters, 3, 4 or 5, provide good clustering. \n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "def kmeans(rfm_log_scaled_df, clusters_number, n_random):    \n",
    "    kmeans = KMeans(n_clusters = clusters_number, random_state = n_random)\n",
    "    kmeans.fit(rfm_log_scaled_df[['recency','frequency','monetary']])\n",
    "    # Extract cluster labels\n",
    "    cluster_labels = kmeans.labels_\n",
    "    cluster_center = kmeans.cluster_centers_\n",
    "    # Create a cluster label column\n",
    "    rfm_log_scaled_df = rfm_log_scaled_df.assign(Cluster = cluster_labels)\n",
    "    \n",
    "    # TSNE is a tool to visualize high-dimensional data\n",
    "    # We will use TSNE to give us a 2-d visualization of the clusters (which are 3-d)\n",
    "    \n",
    "    model = TSNE(random_state=1) # Initialise TSNE\n",
    "    transformed = model.fit_transform(rfm_log_scaled_df[['recency','frequency','monetary']])\n",
    "    # Plot TSNE\n",
    "    plt.title('Flattened Graph of {} Clusters'.format(clusters_number))\n",
    "    sns.scatterplot(x=transformed[:,0], y=transformed[:,1], hue=cluster_labels, style=cluster_labels, palette=\"Set1\")\n",
    "    \n",
    "    return [cluster_labels, cluster_center]\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.subplot(3, 1, 1)\n",
    "labels_k3, centers_k3 = kmeans(rfm_log_scaled, 3, 3)\n",
    "plt.subplot(3, 1, 2)\n",
    "labels_k4, centers_k4 = kmeans(rfm_log_scaled, 4, 4)\n",
    "plt.subplot(3, 1, 3)\n",
    "labels_k5, centers_k5 = kmeans(rfm_log_scaled, 5, 5)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b06c15",
   "metadata": {},
   "source": [
    "Conclusion:\n",
    "\n",
    "The KMEANS of the RFM (log-scaled and fitted) produces 5 well distinguished clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ed9ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the cluster labels to the rfm df:\n",
    "rfm['cluster'] = labels_k5 + 1\n",
    "# let's look at the centroids of the clusters in order to charecterize each of them:\n",
    "display(centers_k5)\n",
    "print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5613eeb",
   "metadata": {},
   "source": [
    "Conclusions:\n",
    "\n",
    "From the centroids we get that\n",
    "* 1: all bad: r-high, m-low, f-low. \n",
    "* 2: all good: r-low, m-high, f-high.\n",
    "* 3: quite good: r-low, f-above avg, m-avg or above avg, \n",
    "* 4: quite bad: r - avg, f-low, m-low. \n",
    "* 5: bad recency: r-high, f-avg, m-avg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20b2cce",
   "metadata": {},
   "source": [
    "<b id=\"-RFM_scores\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b33afd",
   "metadata": {},
   "source": [
    "## Charactarize the product clusters based on R, F and M scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6b47c4",
   "metadata": {},
   "source": [
    "Next we calculate the R, F and M from the quantiles and find the RFM segments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91a6b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_labels=range(2,0, -1) # the R lables are inverted!\n",
    "rfm['R_score'] = pd.qcut(rfm_log_scaled['recency'], q=2, labels=r_labels,  duplicates = 'drop')\n",
    "f_labels=range(1,4+1)\n",
    "rfm['F_score'] = pd.qcut(rfm_log_scaled['frequency'], q=4, labels=f_labels, duplicates = 'drop')\n",
    "m_labels=range(1,4+1)\n",
    "rfm['M_score'] = pd.qcut(rfm_log_scaled['monetary'], q=4, labels=m_labels, duplicates = 'drop')\n",
    "\n",
    "#Let's take a look at the segments we got.\n",
    "rfm['RFM_segment']=rfm['R_score'].astype(str)+rfm['F_score'].astype(str)+rfm['M_score'].astype(str)\n",
    "\n",
    "display(rfm['RFM_segment'].unique())\n",
    "display(rfm['RFM_segment'].nunique())\n",
    "display(rfm['RFM_segment'].value_counts())\n",
    "\n",
    "rfm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d060155",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's plot the RFM segments for each cluster we found with k-means\n",
    "\n",
    "grouped_rfm = rfm.groupby(['cluster'])\n",
    "\n",
    "for name, group in grouped_rfm:\n",
    "    print('cluster: ', name)\n",
    "    display(group)\n",
    "    display(group.RFM_segment.unique())\n",
    "    print()\n",
    "    #display(group.RFM_segment.mode()[0])\n",
    "    print(f'cluster {name} :' , group.RFM_segment.mode()[0] )\n",
    "    print()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fae18f1",
   "metadata": {},
   "source": [
    "# Conclusions:\n",
    "\n",
    "* cluster 2 forms the \"Best Seller\" group: (contains 842 rows) : ['244', '243', '234', '224', '144', '233', '124', '134', '143'] \n",
    "\n",
    "* cluster 1 is  \"Unprofitable\": (contains 551 rows): ['111', '121', '112'] \n",
    "\n",
    "* cluster 3 the \"Potential with Low Recency\": (contains 412 rows) :['123', '112', '122', '143', '133', '132', '121', '144', '134','124', '113', '111', '114', '131', '142']\n",
    "\n",
    "* cluster 4 is \"At Risk - Needs Attention\": (contains 492 rows) : ['221', '222', '121', '122', '112', '111', '211', '123', '212','213', '113']\n",
    "\n",
    "* cluster 5 is \"Avg Up - Prommissing\" (contains 488): 1082 rows: ['123', '223', '122', '233', '133', '232', '132', '234', '224','222', '124', '243', '242', '231', '213', '241', '134', '143','221', '131']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320fb9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_seg_score_dict = {2: \"Best Seller\",\n",
    "                          1: \"Unprofitable\" ,\n",
    "                          3: \"Potential with Low Recency\" ,\n",
    "                          4: \"At Risk - Needs Attention\" ,\n",
    "                          5: \"Avg Up - Prommissing\"}\n",
    "\n",
    "display(cluster_seg_score_dict)\n",
    "rfm['group']=rfm['cluster'].map(cluster_seg_score_dict)\n",
    "display(rfm.head(7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93875738",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Next we count the number of customers in each segment\n",
    "segments_counts = rfm['group'].value_counts().sort_values(ascending=True)\n",
    "\n",
    "# let's bar-plot each cluster's ratio\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "bars = ax.barh(range(len(segments_counts)),\n",
    "              segments_counts,\n",
    "              color='silver')\n",
    "ax.set_frame_on(False)\n",
    "ax.tick_params(left=False,\n",
    "               bottom=False,\n",
    "               labelbottom=False)\n",
    "ax.set_yticks(range(len(segments_counts)))\n",
    "ax.set_yticklabels(segments_counts.index)\n",
    "\n",
    "for i, bar in enumerate(bars):\n",
    "        value = bar.get_width()\n",
    "        if segments_counts.index[i] in ['Best Seller','Avg Up - Prommissing']:\n",
    "            bar.set_color('firebrick')\n",
    "        ax.text(value,\n",
    "                bar.get_y() + bar.get_height()/2,\n",
    "                '{:,} ({:}%)'.format(int(value),\n",
    "                                   int(value*100/segments_counts.sum())),\n",
    "                va='center',\n",
    "                ha='left'\n",
    "               )\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9130107",
   "metadata": {},
   "source": [
    "###### Conclusion:\n",
    "\n",
    "There are a lot of products which are not bought frequently or recently and do not generate a good profit  (16% are unprofitable and 14% are at risk). \n",
    "However, 36% of the items are best sellers or promising (red bars) meaning that they are bought frequently, generate great profit and their recency is also high. \n",
    "32% of the products have potential and with an appropriaate strategy will become even more profitable. Their monetary and freqecy are mostly above average!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74bd769",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.to_csv(file_name, sep='\\t')\n",
    "rfm.to_csv('rfm_final.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b276a4f6",
   "metadata": {},
   "source": [
    "<b id=\"-conclusions\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa4f46c",
   "metadata": {},
   "source": [
    "## Conclusions and Suggestions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48969f54",
   "metadata": {},
   "source": [
    "* Product Description Analysis\n",
    "\n",
    "The number of orders increases from August 2019 until November 2019 which has the best sales considering units, range of product type and revenue. It is suggested to further investigate why November 2018 is the worst month.\n",
    "\n",
    "The leading product in total sales is cake-stand with paper little birdie coming second and hanging heart t-light holder coming third in sales.\n",
    "\n",
    "* Basket Analysis\n",
    "\n",
    "The basket pair with the highest lift (13.3%) is 'rose regency teacup saucer' and 'regency teacup saucer'. Another strong basket pair is: 'bag polkadot' and 'storage bag suki'.\n",
    "The online store can use a similar basket tool to find the top 10 products that resemble items which are missing or to imply marketing strategy on them. It is suggested to also investigate the products with the lowest support or the ones we consider unpopular for other reasons.\n",
    "\n",
    "* RFM Analysis\n",
    "\n",
    "The RFM analysis identified 36% of the products as best sellers or promising meaning that they are bought frequently, generate great profit and their recency is also high. \n",
    "Another 32% of the products have a good potential and and it is suggested to find a strategy to make them more profitable. \n",
    "There are also a lot of products (>30%) which are not bought frequently or recently and do not generate a good profit, these perhaps should removed from the store or require a drastic change in marketing or pricing.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff90030",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rfm\n",
    "#rfm.to_csv('rfm_final.csv')"
   ]
  }
 ],
 "metadata": {
  "ExecuteTimeLog": [
   {
    "duration": 34893,
    "start_time": "2022-04-08T21:18:54.363Z"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
